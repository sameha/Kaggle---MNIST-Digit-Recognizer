{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kaggle - MNIST Digit Recognizer\n",
    "\n",
    "The goal in this competition is to take an image of a handwritten single digit, and determine what that digit is.\n",
    "\n",
    "The code heavily used the [Supervised Torch7 Tutorial](https://github.com/torch/tutorials/tree/master/2_supervised) by **Clement Farabet**, several thanks to him are in order.\n",
    "\n",
    "In the Kaggle competition, CNN got the following results:\n",
    "* 10 training loops got 99.286% for a 148 poistion\n",
    "* 20 training loops got 99.414% for a 114 poistion\n",
    "\n",
    "The project demonstrates how to:\n",
    "* Load and preprocess the data, to facilitate learning\n",
    "* Describe a CNN model to solve a classification task\n",
    "* Choose a loss function to minimize error\n",
    "* Apply an optimization techniques to train the model's parameters\n",
    "* Estimate the model's performance on unseen verification data\n",
    "\n",
    "Resources:\n",
    "- [Classify handwritten digits using the famous MNIST data](https://www.kaggle.com/c/digit-recognizer)\n",
    "- [Machine Learning with Torch7 (Code)](https://github.com/torch/tutorials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==> processing options\t\n",
       "opt.size: how many samples do we load: (small) (10k training, 2k testing) | full (42k training, 28k testing)\t\n",
       "opt.visualize: visualize input data and weights during training: (true) | false\t\n",
       "opt.seed: fixed input seed for repeatable experiments: (1)\t\n",
       "opt.threads: number of threads: (8)\t\n",
       "opt.model: type of model to construct: linear | mlp | (convnet)\t\n",
       "opt.loss: type of loss function to minimize: (nll) | mse | margin\t\n",
       "opt.save: subdirectory to save/log experiments in: (itorch_output)\t\n",
       "opt.plot: live plot: (true) | false\t\n",
       "opt.optimization: optimization method: (SGD) | ASGD | CG | LBFGS\t\n",
       "opt.learningRate: learning rate at t=0: (1e-3)\t\n",
       "opt.batchSize: mini-batch size (1 = pure stochastic): (1)\t\n",
       "opt.weightDecay: weight decay (SGD only): (0)\t\n",
       "opt.momentum: momentum (SGD only): (0)\t\n",
       "opt.t0: start averaging at t0 (ASGD only), in nb of epochs: (1)\t\n",
       "opt.maxIter: maximum nb of iterations for CG and LBFGS: (2)\t\n",
       "opt.type: (double) | float | cuda\t\n",
       "{\n",
       "  weightDecay : 0\n",
       "  maxIter : 2\n",
       "  seed : 1\n",
       "  momentum : 0\n",
       "  t0 : 1\n",
       "  loss : nll\n",
       "  type : double\n",
       "  batchSize : 1\n",
       "  visualize : true\n",
       "  learningRate : 0.001\n",
       "  threads : 8\n",
       "  plot : true\n",
       "  save : ./itorch_output\n",
       "  size : full\n",
       "  optimization : SGD\n",
       "  model : convnet\n",
       "}\n"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "----------------------------------------------------------------------\n",
    "-- This script demonstrates how to load the MNIST digits \n",
    "-- training data, and pre-process it to facilitate learning.\n",
    "--\n",
    "-- The MNIST is a typical example of supervised training dataset.\n",
    "-- The problem to solve is a 10-class classification problem.\n",
    "--\n",
    "-- Sameh Awaida\n",
    "----------------------------------------------------------------------\n",
    "require 'torch'   -- torch\n",
    "require 'image'   -- for color transforms\n",
    "require 'nn'      -- provides a normalization operator\n",
    "require 'paths'   -- checking if file exists\n",
    "require 'xlua'    -- xlua provides useful tools, like progress bars\n",
    "require 'optim'   -- an optimization package, for online and batch methods\n",
    "require 'csvigo'  -- a package to handle CSV files (read and write)\n",
    "\n",
    "----------------------------------------------------------------------\n",
    "opt = {}\n",
    "print '==> processing options'\n",
    "print('opt.size: how many samples do we load: (small) (10k training, 2k testing) | full (42k training, 28k testing)')\n",
    "print('opt.visualize: visualize input data and weights during training: (true) | false')\n",
    "print('opt.seed: fixed input seed for repeatable experiments: (1)')\n",
    "print('opt.threads: number of threads: (8)')\n",
    "print('opt.model: type of model to construct: linear | mlp | (convnet)')\n",
    "print('opt.loss: type of loss function to minimize: (nll) | mse | margin')\n",
    "print('opt.save: subdirectory to save/log experiments in: (itorch_output)')\n",
    "print('opt.plot: live plot: (true) | false')\n",
    "print('opt.optimization: optimization method: (SGD) | ASGD | CG | LBFGS')\n",
    "print('opt.learningRate: learning rate at t=0: (1e-3)')\n",
    "print('opt.batchSize: mini-batch size (1 = pure stochastic): (1)')\n",
    "print('opt.weightDecay: weight decay (SGD only): (0)')\n",
    "print('opt.momentum: momentum (SGD only): (0)')\n",
    "print('opt.t0: start averaging at t0 (ASGD only), in nb of epochs: (1)')\n",
    "print('opt.maxIter: maximum nb of iterations for CG and LBFGS: (2)')\n",
    "print('opt.type: (double) | float | cuda')\n",
    "\n",
    "opt.size = 'full'\n",
    "opt.visualize = true\n",
    "opt.seed = 1\n",
    "opt.threads = 8\n",
    "opt.model = 'convnet'\n",
    "opt.loss = 'nll'\n",
    "opt.save = './itorch_output'\n",
    "opt.plot = true\n",
    "opt.optimization = 'SGD'\n",
    "opt.learningRate = 1e-3\n",
    "opt.batchSize = 1\n",
    "opt.weightDecay = 0\n",
    "opt.momentum = 0\n",
    "opt.t0 = 1\n",
    "opt.maxIter = 2\n",
    "opt.type = 'double'\n",
    "\n",
    "print(opt)\n",
    "\n",
    "-- nb of threads and fixed seed (for repeatable experiments)\n",
    "if opt.type == 'float' then\n",
    "   print('==> switching to floats')\n",
    "   torch.setdefaulttensortype('torch.FloatTensor')\n",
    "elseif opt.type == 'cuda' then\n",
    "   print('==> switching to CUDA')\n",
    "   require 'cunn'\n",
    "   torch.setdefaulttensortype('torch.FloatTensor')\n",
    "end\n",
    "torch.setnumthreads(opt.threads)\n",
    "torch.manualSeed(opt.seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Data\n",
    "\n",
    "The data for this competition were taken from the [MNIST dataset](http://yann.lecun.com/exdb/mnist/index.html). The MNIST (\"Modified National Institute of Standards and Technology\") dataset is a classic within the Machine Learning community that has been extensively studied.\n",
    "\n",
    "Each image is 28 pixels in height and 28 pixels in width, for a total of 784 pixels in total. Each pixel has a single pixel-value associated with it, indicating the lightness or darkness of that pixel, with higher numbers meaning darker. This pixel-value is an integer between 0 and 255, inclusive.\n",
    "\n",
    "The training data set, (train.csv), has 785 columns. The first column, called \"label\", is the digit that was drawn by the user. The rest of the columns contain the pixel-values of the associated image.\n",
    "\n",
    "Each pixel column in the training set has a name like pixelx, where x is an integer between 0 and 783, inclusive. To locate this pixel on the image, suppose that we have decomposed x as x = i * 28 + j, where i and j are integers between 0 and 27, inclusive. Then pixelx is located on row i and column j of a 28 x 28 matrix, (indexing by zero).\n",
    "\n",
    "The test data set, (test.csv), is the same as the training set, except that it does not contain the \"label\" column.\n",
    "\n",
    "Overview of the dataset:\n",
    "\n",
    "- 10 classes, 1 for each digit. Digit '1' has label 1, '9' has label 9 and '0' has label 0.\n",
    "- 42000 digits for training, 28000 digits for testing.\n",
    "- The inputs (images) are 1x28x28 centered around a single digit.\n",
    "- The outputs (targets) are 10-dimensional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==> downloading dataset\t\n",
       "==> loading dataset\t\n",
       "Using full dataset\t\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Print samples of training dataset\t\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "Console does not support images"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 68,
       "width": 204
      }
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       " 2\n",
       " 1\n",
       " 2\n",
       " 5\n",
       " 1\n",
       " 1\n",
       " 8\n",
       " 4\n",
       "[torch.DoubleTensor of size 8]\n",
       "\n",
       "Print samples of testing dataset\t\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "Console does not support images"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 68,
       "width": 204
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print '==> downloading dataset'\n",
    "\n",
    "-- Here we download dataset files. \n",
    "\n",
    "-- Note: files were converted from their original Matlab format\n",
    "-- to Torch's internal format using the mattorch package. The\n",
    "-- mattorch package allows 1-to-1 conversion between Torch and Matlab\n",
    "-- files.\n",
    "\n",
    "-- The MNIST dataset contains 2 files:\n",
    "--    + train: training data\n",
    "--    + test:  test data\n",
    "classes = {'0', '1', '2', '3', '4', '5', '6', '7', '8', '9'}\n",
    "torch_file_train = 'input/train_32x32.t7'\n",
    "torch_file_test = 'input/test_32x32.t7'\n",
    "csv_file_train = 'input/train.csv'\n",
    "csv_file_test = 'input/test.csv'\n",
    "\n",
    "-- Save training tensor variable\n",
    "if (not paths.filep(torch_file_train)) then\n",
    "    if (not paths.filep(csv_file_train)) then\n",
    "        os.execute('wget https://www.kaggle.com/c/digit-recognizer/download/train.csv')\n",
    "    end\n",
    "    local trainSetOriginal = csvigo.load{path=csv_file_train, mode='raw'}\n",
    "    local nTrainingSamples = #trainSetOriginal - 1\n",
    "    local trainFeatures32x32 = torch.Tensor(nTrainingSamples, 1, 32, 32)\n",
    "    local trainLabels = torch.Tensor(nTrainingSamples):zero()\n",
    "    for i = 1, nTrainingSamples do\n",
    "        local labelSample = table.remove(trainSetOriginal[i + 1], 1)\n",
    "        local featuresSample = torch.Tensor(trainSetOriginal[i + 1]):reshape(1, 28, 28)\n",
    "        trainFeatures32x32[i]:narrow(2, 3, 28):narrow(3, 3, 28):copy(featuresSample)\n",
    "        trainLabels[i] = labelSample + 1\n",
    "    end\n",
    "    torch.save(torch_file_train, {trainFeatures32x32, trainLabels})\n",
    "end\n",
    "\n",
    "-- Save testing tensor variable\n",
    "if (not paths.filep(torch_file_test)) then\n",
    "    if (not paths.filep(csv_file_test)) then\n",
    "        os.execute('wget https://www.kaggle.com/c/digit-recognizer/download/test.csv')\n",
    "    end\n",
    "    local testSetOriginal = csvigo.load{path=csv_file_test, mode='raw'}\n",
    "    local nTestSamples = #testSetOriginal - 1\n",
    "    local testFeatures32x32 = torch.Tensor(nTestSamples, 1, 32, 32)\n",
    "    for i = 1, nTestSamples do\n",
    "        local featuresSample = torch.Tensor(testSetOriginal[i + 1]):reshape(1, 28, 28)\n",
    "        testFeatures32x32[i]:narrow(2, 3, 28):narrow(3, 3, 28):copy(featuresSample)\n",
    "    end\n",
    "    torch.save(torch_file_test, testFeatures32x32)\n",
    "end\n",
    "\n",
    "\n",
    "----------------------------------------------------------------------\n",
    "print '==> loading dataset'\n",
    "\n",
    "\n",
    "if (opt.size == 'small') then\n",
    "    train_size = 10000\n",
    "    test_size = 2000\n",
    "    print 'Using small dataset'\n",
    "elseif (opt.size == 'full') then\n",
    "    train_size = 42000\n",
    "    test_size = 28000  \n",
    "    print 'Using full dataset'\n",
    "end\n",
    "\n",
    "-- Load the training dataset\n",
    "raw_data = torch.load(torch_file_train)\n",
    "trainSet = {\n",
    "    data = raw_data[1],\n",
    "    labels = raw_data[2],\n",
    "    size = function() return train_size end\n",
    "}\n",
    "\n",
    "-- Load the testing dataset\n",
    "raw_data = torch.load(torch_file_test)\n",
    "testSet = {\n",
    "    data = raw_data,\n",
    "    size = function() return test_size end\n",
    "}\n",
    "\n",
    "-- Print sample of training and testing set\n",
    "if opt.visualize then\n",
    "    if itorch then\n",
    "        print 'Print samples of training dataset'\n",
    "        first8TrainingSamples = trainSet.data[{ {1,8},{1},{},{}}]\n",
    "        itorch.image(first8TrainingSamples)\n",
    "        print(trainSet.labels[{{1,8}}])\n",
    "        print 'Print samples of testing dataset'\n",
    "        first8TestSamples = testSet.data[{ {1,8},{1},{},{}}]\n",
    "        itorch.image(first8TestSamples)\n",
    "    else print(\"For visualization, run this script in an itorch notebook\") \n",
    "    end\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==> preprocessing data\t\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "==> preprocessing data: normalize each feature (channel) globally\t\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Channel 1 mean: 25.578697614397\t\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Channel 1 Standard Deviation: 70.282630863225\t\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "==> preprocessing data: normalize each channel locally\t\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "----------------------------------------------------------------------\n",
    "print '==> preprocessing data'\n",
    "\n",
    "-- Preprocessing requires a floating point representation (the original\n",
    "-- data is stored on bytes). Types can be easily converted in Torch, \n",
    "-- in general by doing: dst = src:type('torch.TypeTensor'), \n",
    "-- where Type=='Float','Double','Byte','Int',... Shortcuts are provided\n",
    "-- for simplicity (float(),double(),cuda(),...):\n",
    "\n",
    "trainSet.data = trainSet.data:float()\n",
    "testSet.data = testSet.data:float()\n",
    "\n",
    "-- We now preprocess the data. Preprocessing is crucial\n",
    "-- when applying pretty much any kind of machine learning algorithm.\n",
    "-- color channels are normalized globally, across the entire dataset;\n",
    "-- as a result, each color component has 0-mean and 1-norm across the dataset.\n",
    "\n",
    "\n",
    "\n",
    "-- Normalize each channel, and store mean/std\n",
    "-- per channel. These values are important, as they are part of\n",
    "-- the trainable parameters. At test time, test data will be normalized\n",
    "-- using these values.\n",
    "print '==> preprocessing data: normalize each feature (channel) globally'\n",
    "-- normalize each channel globally:\n",
    "mean = trainSet.data[{ {},{1},{},{} }]:mean() -- mean estimation\n",
    "print('Channel 1 mean: ' .. mean)\n",
    "trainSet.data[{ {},{1},{},{} }]:add(-mean) -- mean subtraction\n",
    "testSet.data[{ {},{1},{},{} }]:add(-mean) -- mean subtraction\n",
    "    \n",
    "stdv = trainSet.data[{ {},{1},{},{} }]:std() -- std estimation\n",
    "print('Channel 1 Standard Deviation: ' .. stdv)\n",
    "trainSet.data[{ {},{1},{},{} }]:div(stdv) -- std division\n",
    "testSet.data[{ {},{1},{},{} }]:div(stdv) -- std division\n",
    "\n",
    "-- Local normalization\n",
    "print '==> preprocessing data: normalize each channel locally'\n",
    "\n",
    "-- Define the normalization neighborhood:\n",
    "neighborhood = image.gaussian1D(13)\n",
    "\n",
    "-- Define our local normalization operator (It is an actual nn module,\n",
    "-- which could be inserted into a trainable model):\n",
    "normalization = nn.SpatialContrastiveNormalization(1, neighborhood,1):float()\n",
    "\n",
    "-- Normalize all channels locally:\n",
    "for i = 1,trainSet:size() do\n",
    "    trainSet.data[{ {i},{1},{},{} }] = normalization:forward(trainSet.data[{ {i},{1},{},{} }])\n",
    "end\n",
    "for i = 1,testSet:size() do\n",
    "    testSet.data[{ {i},{1},{},{} }] = normalization:forward(testSet.data[{ {i},{1},{},{} }])\n",
    "end\n",
    "\n",
    "\n",
    "\n",
    "----------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==> verify statistics\t\n"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Training Set Channel 1 mean: -0.030610727006601\t\n"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Testing Set Channel 1 mean: -0.030535941123171\t\n"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Training Set Channel 1 Standard Deviation: 0.58335232858409\t\n"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Testing Set Channel 1 Standard Deviation: 0.58278465218005\t\n",
       "==> visualizing data\t\n"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "Console does not support images"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 374,
       "width": 204
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print '==> verify statistics'\n",
    "-- It's always good practice to verify that data is properly normalized\n",
    "\n",
    "\n",
    "print('Training Set Channel 1 mean: ' .. trainSet.data[{ {},{1},{},{} }]:mean())\n",
    "print('Testing Set Channel 1 mean: ' .. testSet.data[{ {},{1},{},{} }]:mean())\n",
    "print('Training Set Channel 1 Standard Deviation: ' .. trainSet.data[{ {},{1},{},{} }]:std())\n",
    "print('Testing Set Channel 1 Standard Deviation: ' .. testSet.data[{ {},{1},{},{} }]:std())\n",
    "\n",
    "----------------------------------------------------------------------\n",
    "print '==> visualizing data'\n",
    "-- Visualization is quite easy, using itorch.image().\n",
    "\n",
    "if opt.visualize then\n",
    "    if itorch then\n",
    "        first64samples = trainSet.data[{ {1,64},{1},{},{}}]\n",
    "        itorch.image(first64samples)\n",
    "    else print(\"For visualization, run this script in an itorch notebook\") \n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Model Definition\n",
    "\n",
    "In this file, we describe three different models: convolutional neural networks (CNNs, or ConvNets), multi-layer neural networks (MLPs), and a simple linear model (which becomes a logistic regression if used with a negative log-likelihood loss).\n",
    "\n",
    "Linear regression is the simplest type of model. It is parametrized by a weight matrix W, and a bias vector b. Mathematically, it can be written as:\n",
    "$$ y^n = Wx^n+b $$\n",
    "\n",
    "Using the nn package, describing ConvNets, MLPs and other forms of sequential trainable models is really easy. All we have to do is create a top-level wrapper, which, as for the logistic regression, is going to be a sequential module, and then append modules into it. Implementing a simple linear model is therefore trivial:\n",
    "```torch\n",
    "model = nn.Sequential()\n",
    "model:add(nn.Reshape(ninputs))\n",
    "model:add(nn.Linear(ninputs, noutputs))\n",
    "```\n",
    "\n",
    "A slightly more complicated model is the multi-layer neural network (MLP). This model is parametrized by two weight matrices, and two bias vectors:\n",
    "$$ y^n = W_2 \\text{sigmoid}(W_1 x^n + b_1) + b_2 $$\n",
    "\n",
    "where the function sigmoid is typically the symmetric hyperbolic tangent function. Again, in Torch:\n",
    "```torch\n",
    "model = nn.Sequential()\n",
    "model:add(nn.Reshape(ninputs))\n",
    "model:add(nn.Linear(ninputs,nhiddens))\n",
    "model:add(nn.Tanh())\n",
    "model:add(nn.Linear(nhiddens,noutputs))\n",
    "```\n",
    "\n",
    "Compared to the linear regression model, the 2-layer neural network can learn arbitrary non-linear mappings between its inputs and outputs. In practice, it can be quite hard to train fully-connected MLPs to classify natural images.\n",
    "\n",
    "Convolutional Networks are a particular form of MLP, which was tailored to efficiently learn to classify images. Convolutional Networks are trainable architectures composed of multiple stages. The input and output of each stage are sets of arrays called feature maps. For example, if the input is a color image, each feature map would be a 2D array containing a color channel of the input image (for an audio input each feature map would be a 1D array, and for a video or volumetric image, it would be a 3D array). At the output, each feature map represents a particular feature extracted at all locations on the input. \n",
    "\n",
    "Each stage is composed of three layers: a **filter bank layer**, a **non-linearity layer**, and a **feature pooling layer**. A typical ConvNet is composed of one, two or three such 3-layer stages, followed by a **classification module**. Each layer type is now described for the case of image recognition.\n",
    "![Convolutional Networds](itorch_digit_recognizer_cnn.ipynb_img1.png)\n",
    "\n",
    "Trainable hierarchical vision models, and more generally image processing algorithms are usually expressed as sequences of operations or transformations. They can be well described by a modular approach, in which each module processes an input image bank and produces a new bank. The figure above is a nice graphical illustration of this approach. Each module requires the previous bank to be fully (or at least partially) available before computing its output. This causality prevents simple parallelism to be implemented across modules. However parallelism can easily be introduced within a module, and at several levels, depending on the kind of underlying operations. These forms of parallelism are exploited in Torch7.\n",
    "\n",
    "Typical ConvNets rely on a few basic modules:\n",
    "* **Filter bank layer**: the input is a 3D array with n1 2D feature maps of size n2 x n3. Each component is denoted $x_{ijk}$, and each feature map is denoted xi. The output is also a 3D array, y composed of m1 feature maps of size m2 x m3. A trainable filter (kernel) $k_{ij}$ in the filter bank has size l1 x l2 and connects input feature map x to output feature map $y_j$. The module computes $y_j = b_j + i_{kij} * x_i$ where $*$ is the 2D discrete convolution operator and $b_j$ is a trainable bias parameter. Each filter detects a particular feature at every location on the input. Hence spatially translating the input of a feature detection layer will translate the output but leave it otherwise unchanged.\n",
    "* **Non-Linearity Layer**: In traditional ConvNets this simply consists in a pointwise tanh() sigmoid function applied to each site (ijk). However, recent implementations have used more sophisticated non-linearities. A useful one for natural image recognition is the rectified sigmoid Rabs: $|g_i.tanh()|$ where $g_i$ is a trainable gain parameter. The rectified sigmoid is sometimes followed by a subtractive and divisive local normalization N, which enforces local competition between adjacent features in a feature map, and between features at the same spatial location.\n",
    "* **Feature Pooling Layer**: This layer treats each feature map separately. In its simplest instance, it computes the average values over a neighborhood in each feature map. Recent work has shown that more selective poolings, based on the LP-norm, tend to work best, with P=2, or P=inf (also known as max pooling). The neighborhoods are stepped by a stride larger than 1 (but smaller than or equal the pooling neighborhood). This results in a reduced-resolution output feature map which is robust to small variations in the location of features in the previous layer. The average operation is sometimes replaced by a max PM. Traditional ConvNets use a pointwise tanh() after the pooling layer, but more recent models do not. Some ConvNets dispense with the separate pooling layer entirely, but use strides larger than one in the filter bank layer to reduce the resolution. In some recent versions of ConvNets, the pooling also pools similar feature at the same location, in addition to the same feature at nearby locations.\n",
    "\n",
    "A couple of comments about the model that we are going to use:\n",
    "* the input has 3 feature maps, each 32x32 pixels. It is the convention for all nn.Spatial* layers to work on 3D arrays, with the first dimension indexing different features (here normalized YUV), and the next two dimensions indexing the height and width of the image/map.\n",
    "* the fist layer applies 16 filters to the input map, each being 5x5. The receptive field of this first layer is 5x5, and the maps produced by it are therefore 16x28x28. This linear transform is then followed by a non-linearity (tanh), and an L2-pooling function, which pools regions of size 2x2, and uses a stride of 2x2. The result of that operation is a 16x14x14 array, which represents a 14x14 map of 16-dimensional feature vectors. The receptive field of each unit at this stage is 7x7.\n",
    "* the second layer is very much analogous to the first, except that now the 16-dim feature maps are projected into 256- dim maps, with a fully-connected connection table: each unit in the output array is influenced by a 4x5x5 neighborhood of features in the previous layer. That layer has therefore 4x256x5x5 trainable kernel weights (and 256 biases). The result of the complete layer (conv+pooling) is a 256x5x5 array.\n",
    "* at this stage, the 5x5 array of 256-dimensional feature vectors is flattened into a 6400-dimensional vector, which we feed to a two-layer neural net. The final prediction (10-dimensional distribution over classes) is influenced by a 32x32 neighborhood of input variables (YUV pixels).\n",
    "* recent work (Jarret et al.) has demonstrated the advantage of locally normalizing sets of internal features, at each stage of the model. The use of smoother pooling functions, such as the L2 norm for instance instead of the harsher max-pooling, has also been shown to yield better generalization (Sermanet et al.). We use these two ingredients in this model.\n",
    "* one other remark: it is typically not a good idea to use fully connected layers, in internal layers. In general, favoring large numbers of features (over-completeness) over density of connections helps achieve better results (empirical evidence of this was reported in several papers, as in Hadsell et al.). The SpatialConvolutionMap module accepts tables of connectivities (maps) that allows one to create arbitrarily sparse connections between two layers. A couple of standard maps/tables are provided in nn.tables.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==> define parameters\t\n",
       "==> construct model\t\n"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "==> here is the model:\t\n",
       "nn.Sequential {\n",
       "  [input -> (1) -> (2) -> (3) -> (4) -> (5) -> (6) -> (7) -> (8) -> (9) -> (10) -> (11) -> (12) -> output]\n",
       "  (1): nn.SpatialConvolutionMap\n",
       "  (2): nn.Tanh\n",
       "  (3): nn.Sequential {\n",
       "    [input -> (1) -> (2) -> (3) -> (4) -> output]\n",
       "    (1): nn.Square\n",
       "    (2): nn.SpatialAveragePooling(2x2, 2,2)\n",
       "    (3): nn.MulConstant\n",
       "    (4): nn.Sqrt\n",
       "  }\n",
       "  (4): nn.SpatialSubtractiveNormalization\n",
       "  (5): nn.SpatialConvolutionMap\n",
       "  (6): nn.Tanh\n",
       "  (7): nn.Sequential {\n",
       "    [input -> (1) -> (2) -> (3) -> (4) -> output]\n",
       "    (1): nn.Square\n",
       "    (2): nn.SpatialAveragePooling(2x2, 2,2)\n",
       "    (3): nn.MulConstant\n",
       "    (4): nn.Sqrt\n",
       "  }\n",
       "  (8): nn.SpatialSubtractiveNormalization\n",
       "  (9): nn.Reshape(1600)\n",
       "  (10): nn.Linear(1600 -> 128)\n",
       "  (11): nn.Tanh\n",
       "  (12): nn.Linear(128 -> 10)\n",
       "}\n",
       "{\n",
       "  gradInput : DoubleTensor - empty\n",
       "  modules : \n",
       "    {\n",
       "      1 : \n",
       "        nn.SpatialConvolutionMap\n",
       "        {\n",
       "          dH : 1\n",
       "          dW : 1\n",
       "          nInputPlane : 1\n",
       "          kW : 5\n",
       "          gradInput : DoubleTensor - empty\n",
       "          "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "connTable : DoubleTensor - size: 64x2\n",
       "          gradBias : DoubleTensor - size: 64\n",
       "          weight : DoubleTensor - size: 64x5x5\n",
       "          _type : torch.DoubleTensor\n",
       "          gradWeight : DoubleTensor - size: 64x5x5\n",
       "          bias : DoubleTensor - size: 64\n",
       "          nOutputPlane : 64\n",
       "          output : DoubleTensor - empty\n",
       "          kH : 5\n",
       "        }\n",
       "      2 : \n",
       "        nn.Tanh\n",
       "        {\n",
       "          gradInput : DoubleTensor - empty\n",
       "          _type : torch.DoubleTensor\n",
       "          "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "output : DoubleTensor - empty\n",
       "        }\n",
       "      3 : \n",
       "        nn.Sequential {\n",
       "          [input -> (1) -> (2) -> (3) -> (4) -> output]\n",
       "          (1): nn.Square\n",
       "          (2): nn.SpatialAveragePooling(2x2, 2,2)\n",
       "          (3): nn.MulConstant\n",
       "          (4): nn.Sqrt\n",
       "        }\n",
       "        {\n",
       "          dH : 2\n",
       "          dW : 2\n",
       "          _type : torch.DoubleTensor\n",
       "          output : DoubleTensor - empty\n",
       "          gradInput : DoubleTensor - empty\n",
       "          modules : \n",
       "            {\n",
       "              1 : \n",
       "                nn.Square\n",
       "                {\n",
       "                  gradInput : DoubleTensor - empty\n",
       "                  _type : torch.DoubleTensor\n",
       "                  output : DoubleTensor - empty\n",
       "                }\n",
       "              2 : \n",
       "                nn.SpatialAveragePooling(2x2, 2,2)\n",
       "                {\n",
       "                  dH : 2\n",
       "                  dW : 2\n",
       "                  kW : 2\n",
       "                  gradInput : DoubleTensor - empty\n",
       "                  divide : true\n",
       "                  count_include_pad : true\n",
       "                  _type : torch.DoubleTensor\n",
       "                  padH : 0\n",
       "                  ceil_mode : false\n",
       "                  output : DoubleTensor - empty\n",
       "                  kH : 2\n",
       "                  padW : 0\n",
       "                }\n",
       "              3 : \n",
       "                nn.MulConstant\n",
       "                {\n",
       "                  inplace : false\n",
       "                  _type : torch.DoubleTensor\n",
       "                  constant_scalar : 4\n",
       "                  gradInput : DoubleTensor - empty\n",
       "                  output : DoubleTensor - empty\n",
       "                }\n",
       "              4 : \n",
       "                nn.Sqrt\n",
       "                {\n",
       "      "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "            gradInput : DoubleTensor - empty\n",
       "                  eps : 0\n",
       "                  _type : torch.DoubleTensor\n",
       "                  output : DoubleTensor - empty\n",
       "                }\n",
       "            }\n",
       "          kH : 2\n",
       "          kW : 2\n",
       "        }\n",
       "      4 : \n",
       "        nn.SpatialSubtractiveNormalization\n",
       "        {\n",
       "          nInputPlane : 64\n",
       "          divider : \n",
       "            nn.CDivTable\n",
       "            {\n",
       "              gradInput : table: 0x3eaf5070\n",
       "              _type : torch.DoubleTensor\n",
       "              output : DoubleTensor - empty\n",
       "            }\n",
       "          gradInput : DoubleTensor - empty\n",
       "          coef : DoubleTensor - size: 1x1x1\n",
       "          kernel : DoubleTensor - size: 7\n",
       "          _type : torch.DoubleTensor\n",
       "          subtractor : \n",
       "            nn.CSubTable\n",
       "            {\n",
       "              gradInput : table: 0x3eaf3c38\n",
       "              _type : torch.DoubleTensor\n",
       "              output : DoubleTensor - empty\n",
       "            }\n",
       "          meanestimator : \n",
       "            nn.Sequential {\n",
       "              [input -> (1) -> (2) -> (3) -> (4) -> output]\n",
       "              (1): nn.SpatialZeroPadding(l=3, r=3, t=3, b=3)\n",
       "              (2): nn.SpatialConvolutionMap\n",
       "              (3): nn.SpatialConvolution(64 -> 1, 1x7)\n",
       "              (4): nn.Replicate\n",
       "            }\n",
       "            {\n",
       "              gradInput : DoubleTensor - empty\n",
       "              modules : \n",
       "                {\n",
       "                  1 : nn.SpatialZeroPadding(l=3, r=3, t=3, b=3)\n",
       "                  2 : nn.SpatialConvolutionMap\n",
       "                  3 : nn.SpatialConvolution(64 -> 1, 1x7)\n",
       "                  4 : nn.Replicate\n",
       "                }\n",
       "              _type : torch.DoubleTensor\n",
       "              output : DoubleTensor - empty\n",
       "            }\n",
       "          output : DoubleTensor - empty\n",
       "        }\n",
       "      5 : \n",
       "        nn.SpatialConvolutionMap\n",
       "        {\n"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "          dH : 1\n",
       "          dW : 1\n",
       "          nInputPlane : 64\n",
       "          kW : 5\n",
       "          gradInput : DoubleTensor - empty\n",
       "          connTable : DoubleTensor - size: 256x2\n",
       "          gradBias : DoubleTensor - size: 64\n",
       "          weight : DoubleTensor - size: 256x5x5\n",
       "          _type : torch.DoubleTensor\n",
       "          gradWeight : DoubleTensor - size: 256x5x5\n",
       "          bias : DoubleTensor - size: 64\n",
       "          nOutputPlane : 64\n",
       "          output : DoubleTensor - empty\n",
       "          kH : 5\n",
       "        }\n",
       "      6 : \n",
       "        nn.Tanh\n",
       "        {\n",
       "          gradInput : DoubleTensor - empty\n",
       "          _type : torch.DoubleTensor\n",
       "          output : DoubleTensor - empty\n",
       "        }\n",
       "      7 : \n",
       "        nn.Sequential {\n",
       "          [input -> (1) -> (2) -> (3) -> (4) -> output]\n",
       "          (1): nn.Square\n",
       "          (2): nn.SpatialAveragePooling(2x2, 2,2)\n",
       "          (3): nn.MulConstant\n",
       "          (4): nn.Sqrt\n",
       "        }\n",
       "        {\n",
       "          dH : 2\n",
       "          dW : 2\n",
       "         "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       " _type : torch.DoubleTensor\n",
       "          output : DoubleTensor - empty\n",
       "          gradInput : DoubleTensor - empty\n",
       "          modules : \n",
       "            {\n",
       "              1 : \n",
       "                "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "nn.Square\n",
       "                "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "{\n",
       "                  gradInput : DoubleTensor - empty\n",
       "                  _type : torch.DoubleTensor\n",
       "                  output : DoubleTensor - empty\n",
       "                }\n",
       "              2 : \n",
       "                nn.SpatialAveragePooling(2x2, 2,2)\n",
       "                {\n",
       "                  dH : 2\n",
       "                  dW : 2\n",
       "                  kW : 2\n",
       "                  gradInput : DoubleTensor - empty\n",
       "                  divide : true\n",
       "                  count_include_pad : true\n",
       "                  _type : torch.DoubleTensor\n",
       "                  padH : 0\n",
       "                  ceil_mode : false\n",
       "                  output : DoubleTensor - empty\n",
       "                  kH : 2\n",
       "                  padW : 0\n",
       "                }\n",
       "              3 : \n",
       "                nn.MulConstant\n",
       "                {\n",
       "                  inplace : false\n",
       "                  _type : torch.DoubleTensor\n",
       "                  constant_scalar : 4\n",
       "                  gradInput : DoubleTensor - empty\n",
       "                  output : DoubleTensor - empty\n",
       "                }\n",
       "              4 : \n",
       "                nn.Sqrt\n",
       "                {\n",
       "                  gradInput : DoubleTensor - empty\n",
       "                  eps : 0\n",
       "                  _type : torch.DoubleTensor\n",
       "                  output : DoubleTensor - empty\n",
       "                }\n",
       "            }\n",
       "          kH : 2\n",
       "          kW : 2\n",
       "        }\n",
       "      8 : \n",
       "        nn.SpatialSubtractiveNormalization\n",
       "        {\n",
       "          nInputPlane : 64\n",
       "          divider : \n",
       "            nn.CDivTable\n",
       "            {\n",
       "              gradInput : table: 0x3f4d0288\n",
       "              _type : torch.DoubleTensor\n",
       "              output : DoubleTensor - empty\n",
       "            }\n",
       "          gradInput : DoubleTensor - empty\n",
       "          coef : DoubleTensor - size: 1x1x1\n",
       "          kernel : DoubleTensor - size: 7\n",
       "          _type : torch.DoubleTensor\n",
       " "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "         subtractor : \n",
       "            nn.CSubTable\n",
       "            {\n",
       "              gradInput : table: 0x3f4d1448\n",
       "              _type : torch.DoubleTensor\n",
       "              output : DoubleTensor - empty\n",
       "            }\n",
       "          meanestimator : \n",
       "            nn.Sequential {\n",
       "              [input -> (1) -> (2) -> (3) -> (4) -> output]\n",
       "              (1): nn.SpatialZeroPadding(l=3, r=3, t=3, b=3)\n",
       "              (2): nn.SpatialConvolutionMap\n",
       "              (3): nn.SpatialConvolution(64 -> 1, 1x7)\n",
       "              (4): nn.Replicate\n",
       "            }\n",
       "            {\n",
       "              gradInput : DoubleTensor - empty\n",
       "              modules : \n",
       "                {\n",
       "                  1 : nn.SpatialZeroPadding(l=3, r=3, t=3, b=3)\n"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "                  2 : nn.SpatialConvolutionMap\n",
       "                  3 : nn.SpatialConvolution(64 -> 1, 1x7)\n",
       "                  4 : nn.Replicate\n",
       "                }\n",
       "              _type : torch.DoubleTensor\n",
       "              output : DoubleTensor - empty\n",
       "            }\n",
       "          output : DoubleTensor - empty\n",
       "        }\n",
       "      9 : \n",
       "        nn.Reshape(1600)\n",
       "        {\n",
       "          _type : torch.DoubleTensor\n",
       "          output : DoubleTensor - empty\n",
       "          gradInput : DoubleTensor - empty\n",
       "          size : LongStorage - size: 1\n",
       "          nelement : 1600\n",
       "          batchsize : LongStorage - size: 2\n",
       "        }\n",
       "      10 : \n",
       "        nn.Linear(1600 -> 128)\n",
       "        {\n",
       "          gradBias : DoubleTensor - size: 128\n",
       "          weight : DoubleTensor - size: 128x1600\n",
       "          _type : torch.DoubleTensor\n",
       "          output : DoubleTensor - empty\n",
       "          gradInput : DoubleTensor - empty\n",
       "          bias : DoubleTensor - size: 128\n",
       "          gradWeight : DoubleTensor - size: 128x1600\n",
       "        }\n",
       "      11 : \n",
       "        nn.Tanh\n",
       "        {\n",
       "          gradInput : DoubleTensor - empty\n",
       "          _type : torch.DoubleTensor\n",
       "          output : DoubleTensor - empty\n",
       "        }\n",
       "      12 : \n",
       "        nn.Linear(128 -> 10)\n",
       "        {\n",
       "          "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "gradBias : DoubleTensor - size: 10\n",
       "          weight : DoubleTensor - size: 10x128\n",
       "          _type : torch.DoubleTensor\n",
       "          output : DoubleTensor - empty\n",
       "          gradInput : DoubleTensor - empty\n",
       "          bias : DoubleTensor - size: 10\n",
       "          gradWeight : DoubleTensor - size: 10x128\n",
       "        }\n",
       "    }\n",
       "  _type : torch.DoubleTensor\n",
       "  output : DoubleTensor - empty\n",
       "}\n",
       "==> visualizing ConvNet filters\t\n",
       "Normal Convnet Layer 1 filters:\t\n"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "Console does not support images"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 77,
       "width": 42
      }
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Normal Convnet Layer 2 filters:\t\n"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "Console does not support images"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 301,
       "width": 42
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "-- This script demonstrates how to define a couple of different\n",
    "-- models:\n",
    "--   + linear\n",
    "--   + 2-layer neural network (MLP)\n",
    "--   + convolutional network (ConvNet)\n",
    "----------------------------------------------------------------------\n",
    "\n",
    "print '==> define parameters'\n",
    "\n",
    "-- 10 class problem\n",
    "noutputs = 10\n",
    "\n",
    "-- input dimensions\n",
    "nfeats = 1\n",
    "width = 32\n",
    "height = 32\n",
    "ninputs = nfeats * width * height\n",
    "\n",
    "-- number of hidden units (for MLP only):\n",
    "nhiddens = ninputs / 2\n",
    "\n",
    "-- hidden units, filter size (for ConvNets only):\n",
    "nstates = {64,64,128}\n",
    "fanin = {1,4}\n",
    "filtsize = 5\n",
    "poolsize = 2\n",
    "normkernel = image.gaussian1D(7)\n",
    "\n",
    "----------------------------------------------------------------------\n",
    "print '==> construct model'\n",
    "if opt.model == 'linear' then\n",
    "    -- Simple linear model\n",
    "    model = nn.Sequential()\n",
    "    model:add(nn.Reshape(ninputs))\n",
    "    model:add(nn.Linear(ninputs, noutputs))\n",
    "    \n",
    "elseif opt.model == 'mlp' then\n",
    "    -- Simple 2-layer neural network, with tanh hidden units\n",
    "    model = nn.Sequential()\n",
    "    model:add(nn.Reshape(ninputs))\n",
    "    model:add(nn.Linear(ninputs, nhiddens))\n",
    "    model:add(nn.Tanh())\n",
    "    model:add(nn.Linear(nhiddens, noutputs))\n",
    "\n",
    "elseif opt.model == 'convnet' then\n",
    "    if opt.type == 'cuda' then\n",
    "        -- a typical modern convolution network (conv+relu+pool)\n",
    "        model = nn.Sequential()\n",
    "        \n",
    "        -- stage 1 : filter bank -> squashing -> L2 pooling -> normalization\n",
    "        model:add(nn.SpatialConvolutionMM(nfeats, nstates[1], filtsize, filtsize))\n",
    "        model:add(nn.ReLU())\n",
    "        model:add(nn.SpatialMaxPooling(poolsize,poolsize,poolsize,poolsize))\n",
    "        \n",
    "        -- stage 2 : filter bank -> squashing -> L2 pooling -> normalization\n",
    "        model:add(nn.SpatialConvolutionMM(nstates[1], nstates[2], filtsize, filtsize))\n",
    "        model:add(nn.ReLU())\n",
    "        model:add(nn.SpatialMaxPooling(poolsize,poolsize,poolsize,poolsize))\n",
    "        \n",
    "        -- stage 3 : standard 2-layer neural network\n",
    "        model:add(nn.View(nstates[2]*filtsize*filtsize))\n",
    "        model:add(nn.Dropout(0.5))\n",
    "        model:add(nn.Linear(nstates[2]*filtsize*filtsize, nstates[3]))\n",
    "        model:add(nn.ReLU())\n",
    "        model:add(nn.Linear(nstates[3], noutputs))\n",
    "        \n",
    "    else \n",
    "        -- a typical convolutional network, with locally-normalized hidden\n",
    "        -- units, and L2-pooling\n",
    "        -- Note: the architecture of this convnet is loosely based on Pierre Sermanet's\n",
    "        -- work on this dataset (http://arxiv.org/abs/1204.3968). In particular\n",
    "        -- the use of LP-pooling (with P=2) has a very positive impact on\n",
    "        -- generalization. Normalization is not done exactly as proposed in\n",
    "        -- the paper, and low-level (first layer) features are not fed to\n",
    "        -- the classifier.\n",
    "        model = nn.Sequential()\n",
    "        \n",
    "        -- stage 1 : filter bank -> squashing -> L2 pooling -> normalization\n",
    "        model:add(nn.SpatialConvolutionMap(nn.tables.random(nfeats, nstates[1], fanin[1]), filtsize, filtsize))\n",
    "        model:add(nn.Tanh())\n",
    "        model:add(nn.SpatialLPPooling(nstates[1], 2, poolsize, poolsize, poolsize, poolsize))\n",
    "        model:add(nn.SpatialSubtractiveNormalization(nstates[1],normkernel))\n",
    "        \n",
    "        -- stage 2 : filter bank -> squashing -> L2 pooling -> normalization\n",
    "        model:add(nn.SpatialConvolutionMap(nn.tables.random(nstates[1], nstates[2], fanin[2]), filtsize, filtsize))\n",
    "        model:add(nn.Tanh())\n",
    "        model:add(nn.SpatialLPPooling(nstates[2], 2, poolsize, poolsize, poolsize, poolsize))\n",
    "        model:add(nn.SpatialSubtractiveNormalization(nstates[2],normkernel))\n",
    "        \n",
    "        -- stage 3 : standard 2-layer neural network\n",
    "        model:add(nn.Reshape(nstates[2]*filtsize*filtsize))\n",
    "        model:add(nn.Linear(nstates[2]*filtsize*filtsize, nstates[3]))\n",
    "        model:add(nn.Tanh())\n",
    "        model:add(nn.Linear(nstates[3], noutputs))\n",
    "    end\n",
    "else\n",
    "    print('unknown -model')\n",
    "end\n",
    "\n",
    "----------------------------------------------------------------------\n",
    "print '==> here is the model:'\n",
    "print(model)\n",
    "\n",
    "----------------------------------------------------------------------\n",
    "-- Visualization is quite easy, using itorch.image().\n",
    "if opt.visualize == true then\n",
    "    if itorch then\n",
    "        print('==> visualizing ConvNet filters')\n",
    "        if opt.model == 'linear' then\n",
    "            print('Linear Layer 1 filters:')\n",
    "            itorch.image(model.modules[2].weight)\n",
    "        elseif opt.model == 'mlp' then\n",
    "            print('MLP Layer 1 filters:')\n",
    "            itorch.image(model.modules[2].weight)\n",
    "        elseif opt.model == 'convnet' then\n",
    "            if opt.type == 'cuda' then\n",
    "                print('CUDA Convnet Layer 1 filters:')\n",
    "                itorch.image(model.modules[1].weight)\n",
    "                print('CUDA Convnet Layer 2 filters:')\n",
    "                itorch.image(model.modules[4].weight)\n",
    "            else\n",
    "                print('Normal Convnet Layer 1 filters:')\n",
    "                itorch.image(model.modules[1].weight)\n",
    "                print('Normal Convnet Layer 2 filters:')\n",
    "                itorch.image(model.modules[5].weight)\n",
    "            end\n",
    "        else error('Wrong NN Model') end\n",
    "    else print('For visualization, run this script in an itorch notebook') \n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Step 3: Loss Function\n",
    "\n",
    "Now that we have a model, we need to define a loss function to be minimized, across the entire training set:\n",
    "\n",
    "$$ L = \\sum_n l(y^n,t^n) $$\n",
    "\n",
    "One of the simplest loss functions we can minimize is the **mean-square error** between the predictions (outputs of the model), and the groundtruth labels, across the entire dataset:\n",
    "\n",
    "$$ l(y^n,t^n) = \\frac{1}{2} \\sum_i (y_i^n - t_i^n)^2 $$\n",
    "\n",
    "or, in Torch:\n",
    "```torch\n",
    "criterion = nn.MSECriterion()\n",
    "```\n",
    "\n",
    "The MSE loss is typically not a good one for classification, as it forces the model to exactly predict the values imposed by the targets (labels).\n",
    "\n",
    "Instead, a more commonly used, probabilistic objective is the **negative log-likelihood**. To minimize a negative log-likelihood, we first need to turn the predictions of our models into properly normalized log-probabilities. For the linear model, this is achieved by feeding the output units into a softmax function, which turns the linear regression into a logistic regression:\n",
    "\n",
    "$$ P(Y=i|x^n,W,b) = \\text{softmax}(Wx^n+be) $$ \n",
    "$$ P(Y=i|x^n,W,b) = \\frac{ e^{Wx_i^n+b} }{ \\sum_j e^{Wx_j^n+b} } $$\n",
    "\n",
    "As we're interested in classification, the final prediction is then achieved by taking the argmax of this distribution:\n",
    "\n",
    "$$ y^n = \\arg\\max_i P(Y=i|x^n,W,b) $$\n",
    "\n",
    "in which case the ouput y is a scalar.\n",
    "\n",
    "More generally, the output of any model can be turned into normalized log-probabilities, by stacking a softmax function on top. So given any of the models defined above, we can simply do:\n",
    "```torch\n",
    "model:add( nn.LogSoftMax() )\n",
    "```\n",
    "\n",
    "We want to maximize the likelihood of the correct (target) class, for each sample in the dataset. This is equivalent to minimizing the **negative log-likelihood (NLL)**, or minimizing the cross-entropy between the predictions of our model and the targets (training data). Mathematically, the per-sample loss can be defined as:\n",
    "\n",
    "$$ l(x^n,t^n) = -\\log(P(Y=t^n|x^n,W,b)) $$\n",
    "\n",
    "Given that our model already produces log-probabilities (thanks to the softmax), the loss is quite straightforward to estimate. In Torch, we use the ClassNLLCriterion, which expects its input as being a vector of log-probabilities, and the target as being an integer pointing to the correct class:\n",
    "```torch\n",
    "criterion = nn.ClassNLLCriterion()\n",
    "```\n",
    "\n",
    "Finally, another type of classification loss is the **multi-class margin loss**, which is closer to the well-known SVM loss. This loss function doesn't require normalized outputs, and can be implemented like this:\n",
    "```torch\n",
    "criterion = nn.MultiMarginCriterion()\n",
    "```\n",
    "\n",
    "The margin loss typically works on par with the negative log-likelihood. I haven't tested this thoroughly, so it's time for more exercises.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==> define loss\t\n",
       "==> here is the loss function:\t\n",
       "nll\t\n"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "----------------------------------------------------------------------\n",
    "-- This script demonstrates how to define a couple of different\n",
    "-- loss functions:\n",
    "--   + negative-log likelihood, using log-normalized output units (SoftMax)\n",
    "--   + mean-square error\n",
    "--   + margin loss (SVM-like)\n",
    "----------------------------------------------------------------------\n",
    "\n",
    "print '==> define loss'\n",
    "\n",
    "if opt.loss == 'margin' then\n",
    "    -- This loss takes a vector of classes, and the index of\n",
    "    -- the grountruth class as arguments. It is an SVM-like loss\n",
    "    -- with a default margin of 1.\n",
    "    criterion = nn.MultiMarginCriterion()\n",
    "\n",
    "elseif opt.loss == 'nll' then\n",
    "    -- This loss requires the outputs of the trainable model to\n",
    "    -- be properly normalized log-probabilities, which can be\n",
    "    -- achieved using a softmax function\n",
    "    model:add(nn.LogSoftMax())\n",
    "    \n",
    "    -- The loss works like the MultiMarginCriterion: it takes\n",
    "    -- a vector of classes, and the index of the grountruth class\n",
    "    -- as arguments.\n",
    "    criterion = nn.ClassNLLCriterion()\n",
    "\n",
    "elseif opt.loss == 'mse' then    \n",
    "    -- The mean-square error is not recommended for classification\n",
    "    -- tasks, as it typically tries to do too much, by exactly modeling\n",
    "    -- the 1-of-N distribution. For the sake of showing more examples,\n",
    "    -- we still provide it here:\n",
    "    criterion = nn.MSECriterion()\n",
    "    \n",
    "    -- Compared to the other losses, the MSE criterion needs a distribution\n",
    "    -- as a target, instead of an index. Indeed, it is a regression loss!\n",
    "    -- So we need to transform the entire label vectors:\n",
    "    if trainSet then\n",
    "        -- convert training labels:\n",
    "        local trsize = trainSet:size()\n",
    "        local trlabels = torch.Tensor(trsize, noutputs)\n",
    "        trlabels:fill(-1)\n",
    "        for i = 1,trsize do trlabels[{i,trainSet.labels[i]}] = 1 end\n",
    "        trainSet.labels = trlabels\n",
    "        \n",
    "        -- convert training labels:\n",
    "        local tesize = testSet:size()\n",
    "        local telabels = torch.Tensor(tesize, noutputs)\n",
    "        telabels:fill(-1)\n",
    "        for i = 1,tesize do telabels[{i,testSet.labels[i]}] = 1 end\n",
    "        testSet.labels = telabels\n",
    "    end\n",
    "\n",
    "else \n",
    "    error('error: unknown -class')\n",
    "end\n",
    "\n",
    "----------------------------------------------------------------------\n",
    "print '==> here is the loss function:'\n",
    "print(opt.loss)\n",
    "--print(criterion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Training Procedure\n",
    "\n",
    "We now have some training data, a model to train, and a loss function to minimize. We define a training procedure.\n",
    "\n",
    "A very important aspect about supervised training of non-linear models (ConvNets and MLPs) is the fact that the optimization problem is not convex anymore. This reinforces the need for a stochastic estimation of gradients, which have shown to produce much better generalization results for several problems.\n",
    "\n",
    "In this example, we show how the optimization algorithm can be easily set to either L-BFGS, CG, SGD or ASGD. In practice, it's very important to start with a few epochs of pure SGD, before switching to L-BFGS or ASGD (if switching at all). The intuition for that is related to the non-convex nature of the problem: at the very beginning of training (random initialization), the landscape might be highly non-convex, and no assumption should be made about the shape of the energy function. Often, SGD is the best we can do. Later on, batch methods (L-BFGS, CG) can be used more safely.\n",
    "\n",
    "Interestingly, in the case of large convex problems, stochasticity is also very important, as it allows much faster (rough) convergence. Several works have explored these techniques, in particular, this recent paper from Byrd/Nocedal, and work on pure stochastic gradient descent by Bottou.\n",
    "\n",
    "Here is our full training function, which demonstrates that you can switch the optimization you're using at runtime (if you want to), and also modify the batch size you're using at run time. You can do all these things because we create the evaluation closure each time we create a new batch. If the batch size is 1, then the method is purely stochastic. If the batch size is set to the complete dataset, then the method is a pure batch method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==> defining some tools\t\n"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "----------------------------------------------------------------------\n",
    "-- This script demonstrates how to define a training procedure,\n",
    "-- irrespective of the model/loss functions chosen.\n",
    "--\n",
    "-- It shows how to:\n",
    "--   + construct mini-batches on the fly\n",
    "--   + define a closure to estimate (a noisy) loss\n",
    "--     function, as well as its derivatives wrt the parameters of the\n",
    "--     model to be trained\n",
    "--   + optimize the function, according to several optmization\n",
    "--     methods: SGD, L-BFGS.\n",
    "----------------------------------------------------------------------\n",
    "\n",
    "-- CUDA?\n",
    "if opt.type == 'cuda' then\n",
    "    model:cuda()\n",
    "    criterion:cuda()\n",
    "end\n",
    "\n",
    "----------------------------------------------------------------------\n",
    "print '==> defining some tools'\n",
    "\n",
    "-- This matrix records the current confusion across classes\n",
    "confusion = optim.ConfusionMatrix(classes)\n",
    "\n",
    "-- Log results to files\n",
    "trainLogger = optim.Logger(paths.concat(opt.save, 'train.log'))\n",
    "\n",
    "-- Retrieve parameters and gradients:\n",
    "-- this extracts and flattens all the trainable parameters of the mode\n",
    "-- into a 1-dim vector\n",
    "if model then\n",
    "    parameters,gradParameters = model:getParameters()\n",
    "end\n",
    "\n",
    "----------------------------------------------------------------------\n",
    "print '==> configuring optimizer'\n",
    "\n",
    "if opt.optimization == 'CG' then\n",
    "    optimState = {maxIter = opt.maxIter}\n",
    "    optimMethod = optim.cg\n",
    "    \n",
    "elseif opt.optimization == 'LBFGS' then\n",
    "    optimState = {\n",
    "        learningRate = opt.learningRate,\n",
    "        maxIter = opt.maxIter,\n",
    "        nCorrection = 10\n",
    "    }\n",
    "    optimMethod = optim.lbfgs\n",
    "\n",
    "elseif opt.optimization == 'SGD' then\n",
    "    optimState = {\n",
    "        learningRate = opt.learningRate,\n",
    "        weightDecay = opt.weightDecay,\n",
    "        momentum = opt.momentum,\n",
    "        learningRateDecay = 1e-17\n",
    "    }\n",
    "    optimMethod = optim.sgd\n",
    "    \n",
    "elseif opt.optimization == 'ASGD' then\n",
    "    optimState = {\n",
    "        eta0 = opt.learningRate,\n",
    "        t0 = train_size * opt.t0\n",
    "    }\n",
    "    optimMethod = optim.asgd\n",
    "else error('unknown optimization method')\n",
    "end\n",
    "\n",
    "----------------------------------------------------------------------\n",
    "print '==> defining training procedure'\n",
    "\n",
    "function train()\n",
    "    -- Printing training size\n",
    "    print('==> Training on ' .. trainSet:size() .. ' samples')\n",
    "    -- epoch tracker\n",
    "    epoch = epoch or 1\n",
    "    \n",
    "    -- local vars\n",
    "    local time = sys.clock()\n",
    "    \n",
    "    -- set model to training mode (for modules that differ in training and testing, like Dropout)\n",
    "    model:training()\n",
    "    \n",
    "    -- shuffle at each epoch\n",
    "    shuffle = torch.randperm(train_size)\n",
    "    \n",
    "    -- do one epoch\n",
    "    print('==> doing epoch on training data:')\n",
    "    print(\"==> online epoch # \" .. epoch .. ' [batchSize = ' .. opt.batchSize .. ']')\n",
    "    \n",
    "    for t = 1,trainSet:size(), opt.batchSize do\n",
    "        -- disp progress\n",
    "        xlua.progress(t, trainSet:size())\n",
    "        \n",
    "        -- create mini batch\n",
    "        local inputs = {}\n",
    "        local targets = {}\n",
    "        \n",
    "        for i = t,math.min(t+opt.batchSize-1, trainSet:size()) do\n",
    "            -- load new sample\n",
    "            local input = trainSet.data[shuffle[i]]\n",
    "            local target = trainSet.labels[shuffle[i]]\n",
    "            \n",
    "            if opt.type == 'double' then\n",
    "                input = input:double()\n",
    "                if opt.loss == 'mse' then target = target:double() end\n",
    "            elseif opt.type == 'cuda' then\n",
    "                input = input:cuda()\n",
    "                if opt.loss == 'mse' then target = target:cuda() end\n",
    "            end\n",
    "            \n",
    "            table.insert(inputs,input)\n",
    "            table.insert(targets, target)\n",
    "        end\n",
    "        \n",
    "        -- create closure to evaluate f(X) and df/dX\n",
    "        local feval = function(x)\n",
    "            -- get new parameters\n",
    "            if x ~= parameters then parameters:copy(x) end\n",
    "            \n",
    "            -- reset gradients\n",
    "            gradParameters:zero()\n",
    "            \n",
    "             -- f is the average of all criterions\n",
    "            local f = 0\n",
    "            \n",
    "            -- evaluate function for complete mini batch\n",
    "            for i = 1,#inputs do\n",
    "                -- estimate f\n",
    "                local output = model:forward(inputs[i])\n",
    "                local err = criterion:forward(output, targets[i])\n",
    "                f = f + err\n",
    "                \n",
    "                -- estimate df/dW\n",
    "                local df_dw = criterion:backward(output, targets[i])\n",
    "                model:backward(inputs[i], df_dw)\n",
    "                \n",
    "                -- update confusion\n",
    "                confusion:add(output, targets[i])\n",
    "            end\n",
    "            \n",
    "            -- normalize gradients and f(X)\n",
    "            gradParameters:div(#inputs)\n",
    "            f = f/#inputs\n",
    "            \n",
    "            -- return f and df/dX\n",
    "            return f, gradParameters\n",
    "        end\n",
    "        \n",
    "        -- optimize on current mini-batch\n",
    "        if optimMethod == optim.asgd then _,_,average = optimMethod(feval, parameters, optimState)\n",
    "        else optimMethod(feval, parameters, optimState)\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    -- time taken\n",
    "    time = sys.clock() - time\n",
    "    time = time / trainSet:size()\n",
    "    print(\"\\n==> time to learn 1 sample = \" .. (time*1000) .. 'ms')\n",
    "    \n",
    "    -- print confusion matrix\n",
    "    print(confusion)\n",
    "    \n",
    "    -- update logger/plot\n",
    "    trainLogger:add{['% mean class accuracy (train set)'] = confusion.totalValid * 100}\n",
    "    if opt.plot then\n",
    "        trainLogger:style{['% mean class accuracy (train set)'] = '-'}\n",
    "        trainLogger:plot()\n",
    "    end\n",
    "    \n",
    "    -- save/log current net\n",
    "    local filename = paths.concat(opt.save, 'model.net')\n",
    "    --os.execute('mkdir -p ' .. sys.dirname(filename))\n",
    "    print('==> saving model to '..filename)\n",
    "    torch.save(filename, model)\n",
    "    \n",
    "    -- next epoch\n",
    "    confusion:zero()\n",
    "    epoch = epoch + 1\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Test the Model\n",
    "\n",
    "A common thing to do is to test the model's performance while we train it. Usually, this test is done on a subset of the training data, that is kept for validation. Here we simply define the test procedure on the available test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==> defining test procedure\t\n"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "----------------------------------------------------------------------\n",
    "-- This script implements save the test result to CSV file\n",
    "----------------------------------------------------------------------\n",
    "\n",
    "print '==> defining test procedure'\n",
    "\n",
    "-- test function\n",
    "function test()\n",
    "    -- print number of testing samples\n",
    "    print('==> Testing on ' .. testSet:size() .. ' samples')\n",
    "    local csv_file_output = paths.concat(opt.save, 'output.csv')\n",
    "    -- local vars\n",
    "    local time = sys.clock()\n",
    "    \n",
    "    -- averaged param use?\n",
    "    if average then\n",
    "        cachedparams = parameters:clone()\n",
    "        parameters:copy(average)\n",
    "    end\n",
    "    \n",
    "    -- set model to evaluate mode (for modules that differ in training and testing, like Dropout)\n",
    "    model:evaluate()\n",
    "    \n",
    "    -- test over test data\n",
    "    print('==> Writing prediction to CSV file:')\n",
    "    local csv_data = {}\n",
    "    csv_data[1] = {'ImageId','Label'}\n",
    "\n",
    "    for t = 1,testSet:size() do\n",
    "        -- disp progress\n",
    "        xlua.progress(t, testSet:size())\n",
    "        \n",
    "        -- get new sample\n",
    "        local input = testSet.data[t]\n",
    "        if opt.type == 'double' then input = input:double()\n",
    "        elseif opt.type == 'cuda' then input = input:cuda() end\n",
    "        \n",
    "        -- test sample\n",
    "        local pred = model:forward(input)\n",
    "        local _, indices = torch.sort(pred, true) \n",
    "        csv_data[t+1] = {}\n",
    "        csv_data[t+1][1] = t\n",
    "        csv_data[t+1][2] = indices[1] - 1\n",
    "    end\n",
    "    \n",
    "    -- timing\n",
    "    time = sys.clock() - time\n",
    "    time = time / testSet:size()\n",
    "    print(\"\\n==> time to test 1 sample = \" .. (time*1000) .. 'ms')\n",
    "    \n",
    "    -- print confusion matrix\n",
    "    csvigo.save{path=csv_file_output,data = csv_data}\n",
    "    \n",
    "    -- averaged param use?\n",
    "    if average then \n",
    "        -- restore parameters\n",
    "        parameters:copy(cachedparams)\n",
    "    end\n",
    "    \n",
    "    -- next iteration:\n",
    "    confusion:zero()\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==> training!\t\n",
       "==> Training on 42000 samples\t\n"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      " [..............................................]                               \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b ETA: 0ms | Step: 0ms\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b "
     ]
    },
    {
     "data": {
      "text/plain": [
       "==> doing epoch on training data:\t\n",
       "==> online epoch # 1 [batchSize = 1]\t\n"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [=============================================>]                               \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b Tot: 8m32s | Step: 11ms\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b "
     ]
    },
    {
     "data": {
      "text/plain": [
       "\n",
       "==> time to learn 1 sample = 12.197138547897ms\t\n",
       "ConfusionMatrix:\n",
       "[[    4030      15       6      22       1      13      23       5      15       2]   97.531% \t[class: 0]\n",
       " [       3    4611      37       5       4       1       8      13       2       0]   98.442% \t[class: 1]\n",
       " [      32     101    3838      63      10       6      15      72      35       5]   91.884% \t[class: 2]\n",
       " [      19      54      31    4126       1      33       1      41      31      14]   94.829% \t[class: 3]\n",
       " [      24     108       6       7    3793       0      15      33       9      77]   93.148% \t[class: 4]\n",
       " [      26      68       1     149       1    3486      25       6      14      19]   91.858% \t[class: 5]\n",
       " [      83      67       0      10       7      18    3934       1      16       1]   95.093% \t[class: 6]\n",
       " [       7      74      47      13      14       2       0    4152       8      84]   94.342% \t[class: 7]\n",
       " [      38     131      25      89      17      24      34      29    3614      62]   88.949% \t[class: 8]\n",
       " [      39      62       3      56      45       6       1     167      31    3778]]  90.210% \t[class: 9]\n",
       " + average row correct: 93.628634810448% \n",
       " + average rowUcol correct (VOC measure): 88.287142515182% \n",
       " + global correct: 93.719047619048%\n",
       "{\n",
       "  _mat_flat : LongTensor - size: 100\n",
       "  valids : FloatTensor - size: 10\n",
       "  mat : LongTensor - size: 10x10\n",
       "  averageUnionValid : 0.88287142515182\n",
       "  _targ_idx : LongTensor - empty\n",
       "  averageValid : 0.93628634810448\n",
       "  classes : \n",
       "    {\n",
       "      1 : 0\n",
       "      2 : 1\n",
       "      3 : 2\n",
       "      4 : 3\n",
       "      5 : 4\n",
       "      6 : 5\n",
       "      7 : 6\n",
       "      8 : 7\n",
       "      9 : 8\n",
       "      10 : 9\n",
       "    }\n",
       "  _prediction : FloatTensor - size: 10\n",
       "  _pred_idx : LongTensor - size: 1\n",
       "  nclasses : 10\n",
       "  _max : FloatTensor - size: 1\n"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "  _target : FloatTensor - empty\n",
       "  unionvalids : FloatTensor - size: 10\n",
       "  totalValid : 0.93719047619048\n",
       "}\n"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "==> saving model to /Users/sameh/Downloads/My Work/Kaggle - MNIST Digit Recognizer/itorch_output/model.net\t\n"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "==> Training on 42000 samples\t\n"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "==> doing epoch on training data:\t\n",
       "==> online epoch # 2 [batchSize = 1]\t\n"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [=============================================>]                               \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b Tot: 8m20s | Step: 11ms\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b "
     ]
    },
    {
     "data": {
      "text/plain": [
       "\n",
       "==> time to learn 1 sample = 11.948729526429ms\t\n",
       "ConfusionMatrix:\n",
       "[[    4103       0       3       1       1       3      13       0       8       0]   99.298% \t[class: 0]\n",
       " [       1    4638      25       2       4       0       0      10       4       0]   99.018% \t[class: 1]\n",
       " [       2      12    4105      11       4       0       2      20      17       4]   98.276% \t[class: 2]\n",
       " [       5       0      15    4265       0      22       0      14      20      10]   98.023% \t[class: 3]\n",
       " [       2       8       5       0    4010       0       4       8       2      33]   98.477% \t[class: 4]\n",
       " [       5       2       3      11       1    3744      15       1       8       5]   98.656% \t[class: 5]\n",
       " [       9       3       2       1       4       9    4097       0      12       0]   99.033% \t[class: 6]\n",
       " [       2       6      26       2      11       1       0    4327       7      19]   98.319% \t[class: 7]\n",
       " [       6       6      17       7      10       7       7       2    3971      30]   97.736% \t[class: 8]\n",
       " [      13       3       1      14      29       9       0      32      17    4070]]  97.182% \t[class: 9]\n",
       " + average row correct: 98.401911854744% \n",
       " + average rowUcol correct (VOC measure): 96.855754256248% \n",
       " + global correct: 98.404761904762%\n",
       "{\n",
       "  _mat_flat : LongTensor - size: 100\n",
       "  valids : FloatTensor - size: 10\n",
       "  mat : LongTensor - size: 10x10\n",
       "  averageUnionValid : 0.96855754256248\n",
       "  _targ_idx : LongTensor - empty\n",
       "  averageValid : 0.98401911854744\n",
       "  classes : \n",
       "    {\n",
       "      1 : 0\n"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "      2 : 1\n",
       "      3 : 2\n",
       "      4 : 3\n",
       "      5 : 4\n",
       "      6 : 5\n",
       "      7 : 6\n",
       "      8 : 7\n",
       "      9 : 8\n",
       "      10 : 9\n",
       "    }\n",
       "  _prediction : FloatTensor - size: 10\n",
       "  _pred_idx : LongTensor - size: 1\n",
       "  nclasses : 10\n",
       "  _max : FloatTensor - size: 1\n",
       "  _target : FloatTensor - empty\n",
       "  unionvalids : FloatTensor - size: 10\n",
       "  totalValid : 0.98404761904762\n",
       "}\n"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "==> saving model to /Users/sameh/Downloads/My Work/Kaggle - MNIST Digit Recognizer/itorch_output/model.net\t\n"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "==> Training on 42000 samples\t\n",
       "==> doing epoch on training data:\t\n",
       "==> online epoch # 3 [batchSize = 1]\t\n"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [=============================================>]                               \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b Tot: 8m23s | Step: 12ms\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b "
     ]
    },
    {
     "data": {
      "text/plain": [
       "\n",
       "==> time to learn 1 sample = 12.012469002179ms\t\n"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "ConfusionMatrix:\n",
       "[[    4110       0       3       0       0       1      12       0       4       2]   99.468% \t[class: 0]\n",
       " [       0    4654      13       1       2       1       1       7       4       1]   99.360% \t[class: 1]\n",
       " [       2      10    4122      10       3       0       1      15      12       2]   98.683% \t[class: 2]\n",
       " [       3       0      13    4289       0      19       0       7      13       7]   98.575% \t[class: 3]\n",
       " [       0       4       3       0    4031       0       5       4       1      24]   98.993% \t[class: 4]\n",
       " [       3       2       1       7       1    3753      12       1      10       5]   98.893% \t[class: 5]\n",
       " [       8       2       2       1       2       5    4107       0      10       0]   99.275% \t[class: 6]\n",
       " [       0       7      20       5       8       1       0    4345       5      10]   98.728% \t[class: 7]\n",
       " [       5       5      15      10       8       4       8       3    3989      16]   98.179% \t[class: 8]\n",
       " [       9       3       2       4      19       5       0      22      15    4109]]  98.114% \t[class: 9]\n",
       " + average row correct: 98.826653957367% \n",
       " + average rowUcol correct (VOC measure): 97.683081030846% \n",
       " + global correct: 98.830952380952%\n",
       "{\n",
       "  _mat_flat : LongTensor - size: 100\n",
       "  valids : FloatTensor - size: 10\n",
       "  mat : LongTensor - size: 10x10\n",
       "  averageUnionValid : 0.97683081030846\n",
       "  _targ_idx : LongTensor - empty\n",
       "  averageValid : 0.98826653957367\n",
       "  classes : \n",
       "    {\n",
       "      1 : 0\n",
       "      2 : 1\n",
       "      3 : 2\n",
       "      4 : 3\n",
       "      5 : 4\n",
       "      6 : 5\n",
       "      7 : 6\n",
       "      8 : 7\n",
       "      9 : 8\n",
       "      10 : 9\n",
       "    }\n",
       "  _prediction : FloatTensor - size: 10\n",
       "  _pred_idx : LongTensor - size: 1\n",
       "  nclasses : 10\n",
       "  _max : FloatTensor - size: 1\n",
       "  _target : FloatTensor - empty\n",
       "  unionvalids : FloatTensor - size: 10\n",
       "  totalValid : 0.98830952380952\n",
       "}\n"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "==> saving model to /Users/sameh/Downloads/My Work/Kaggle - MNIST Digit Recognizer/itorch_output/model.net\t\n"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "==> Training on 42000 samples\t\n",
       "==> doing epoch on training data:\t\n",
       "==> online epoch # 4 [batchSize = 1]\t\n"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [=============================================>]                               \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b Tot: 8m20s | Step: 11ms\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b "
     ]
    },
    {
     "data": {
      "text/plain": [
       "\n",
       "==> time to learn 1 sample = 11.950023645446ms\t\n"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "ConfusionMatrix:\n",
       "[[    4116       0       2       0       0       1       8       0       3       2]   99.613% \t[class: 0]\n",
       " [       1    4659       7       2       2       0       2       8       3       0]   99.466% \t[class: 1]\n",
       " [       1       6    4141       5       1       0       1      13       6       3]   99.138% \t[class: 2]\n",
       " [       2       0      11    4304       0      14       0       5       8       7]   98.920% \t[class: 3]\n",
       " [       2       2       2       0    4036       1       5       4       3      17]   99.116% \t[class: 4]\n",
       " [       3       0       2       6       0    3762      14       0       6       2]   99.130% \t[class: 5]\n",
       " [       5       2       2       1       2       6    4113       0       6       0]   99.420% \t[class: 6]\n",
       " [       0       6      17       2       5       0       0    4362       2       7]   99.114% \t[class: 7]\n",
       " [       4       4      10       5       4       6       4       1    4012      13]   98.745% \t[class: 8]\n",
       " [       7       3       1       2      19       6       0      16      13    4121]]  98.400% \t[class: 9]\n",
       " + average row correct: 99.106199145317% \n",
       " + average rowUcol correct (VOC measure): 98.227244019508% \n",
       " + global correct: 99.109523809524%\n",
       "{\n",
       "  _mat_flat : LongTensor - size: 100\n",
       "  valids : FloatTensor - size: 10\n",
       "  mat : LongTensor - size: 10x10\n",
       "  averageUnionValid : 0.98227244019508\n",
       "  _targ_idx : LongTensor - empty\n",
       "  averageValid : 0.99106199145317\n",
       "  classes : \n",
       "    {\n",
       "      1 : 0\n",
       "      2 : 1\n",
       "      3 : 2\n",
       "      4 : 3\n",
       "      5 : 4\n",
       "      6 : 5\n",
       "      7 : 6\n",
       "      8 : 7\n",
       "      9 : 8\n",
       "      10 : 9\n",
       "    }\n",
       "  _prediction : FloatTensor - size: 10\n",
       "  _pred_idx : LongTensor - size: 1\n",
       "  nclasses : 10\n",
       "  _max : FloatTensor - size: 1\n",
       "  _target : FloatTensor - empty\n",
       "  unionvalids : FloatTensor - size: 10\n",
       "  totalValid : 0.99109523809524\n",
       "}\n"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "==> saving model to /Users/sameh/Downloads/My Work/Kaggle - MNIST Digit Recognizer/itorch_output/model.net\t\n"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "==> Training on 42000 samples\t\n",
       "==> doing epoch on training data:\t\n",
       "==> online epoch # 5 [batchSize = 1]\t\n"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [=============================================>]                               \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b Tot: 7m57s | Step: 12ms\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b "
     ]
    },
    {
     "data": {
      "text/plain": [
       "\n",
       "==> time to learn 1 sample = 11.398810142563ms\t\n"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "ConfusionMatrix:\n",
       "[[    4120       0       2       0       0       0       5       0       3       2]   99.710% \t[class: 0]\n",
       " [       0    4664       6       1       2       0       2       7       2       0]   99.573% \t[class: 1]\n",
       " [       0       3    4149       4       3       0       0      12       5       1]   99.330% \t[class: 2]\n",
       " [       2       0       7    4312       0      11       0       6       8       5]   99.104% \t[class: 3]\n",
       " [       0       2       1       0    4044       0       4       5       2      14]   99.312% \t[class: 4]\n",
       " [       2       0       1       7       0    3771       7       0       4       3]   99.368% \t[class: 5]\n",
       " [       6       2       1       1       1       5    4114       0       7       0]   99.444% \t[class: 6]\n",
       " [       0       6      13       0       5       0       0    4368       1       8]   99.250% \t[class: 7]\n",
       " [       2       3       8       5       3       3       4       2    4021      12]   98.966% \t[class: 8]\n",
       " [       4       2       2       0      12       4       0      15      12    4137]]  98.782% \t[class: 9]\n",
       " + average row correct: 99.28386092186% \n",
       " + average rowUcol correct (VOC measure): 98.578516244888% \n",
       " + global correct: 99.285714285714%\n",
       "{\n",
       "  _mat_flat : LongTensor - size: 100\n",
       "  valids : FloatTensor - size: 10\n",
       "  mat : LongTensor - size: 10x10\n",
       "  averageUnionValid : 0.98578516244888\n",
       "  _targ_idx : LongTensor - empty\n",
       "  averageValid : 0.9928386092186\n",
       "  classes : \n",
       "    {\n",
       "      1 : 0\n",
       "      2 : 1\n",
       "      3 : 2\n",
       "      4 : 3\n",
       "      5 : 4\n",
       "      6 : 5\n",
       "      7 : 6\n",
       "      8 : 7\n",
       "      9 : 8\n",
       "      10 : 9\n",
       "    }\n",
       "  _prediction : FloatTensor - size: 10\n",
       "  _pred_idx : LongTensor - size: 1\n",
       "  nclasses : 10\n",
       "  _max : FloatTensor - size: 1\n",
       "  _target : FloatTensor - empty\n",
       "  unionvalids : FloatTensor - size: 10\n",
       "  totalValid : 0.99285714285714\n",
       "}\n"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "==> saving model to /Users/sameh/Downloads/My Work/Kaggle - MNIST Digit Recognizer/itorch_output/model.net\t\n"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "==> Training on 42000 samples\t\n"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "==> doing epoch on training data:\t\n",
       "==> online epoch # 6 [batchSize = 1]\t\n"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [=============================================>]                               \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b Tot: 8m6s | Step: 11ms\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b "
     ]
    },
    {
     "data": {
      "text/plain": [
       "\n",
       "==> time to learn 1 sample = 11.615440425419ms\t\n"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "ConfusionMatrix:\n",
       "[[    4124       0       0       0       1       0       4       0       1       2]   99.806% \t[class: 0]\n",
       " [       0    4664       6       0       2       0       2       6       4       0]   99.573% \t[class: 1]\n",
       " [       0       4    4154       4       1       0       0       9       4       1]   99.449% \t[class: 2]\n",
       " [       2       0       4    4323       0       6       0       4       6       6]   99.356% \t[class: 3]\n",
       " [       0       1       0       0    4052       0       3       2       0      14]   99.509% \t[class: 4]\n",
       " [       1       0       0       2       0    3778       6       0       6       2]   99.552% \t[class: 5]\n",
       " [       4       1       1       1       1       5    4118       0       6       0]   99.541% \t[class: 6]\n",
       " [       0       7      11       1       5       0       0    4369       3       5]   99.273% \t[class: 7]\n",
       " [       2       1       8       3       3       6       1       1    4025      13]   99.065% \t[class: 8]\n",
       " [       5       1       1       3       9       3       0      11       8    4147]]  99.021% \t[class: 9]\n",
       " + average row correct: 99.414548873901% \n",
       " + average rowUcol correct (VOC measure): 98.833605647087% \n",
       " + global correct: 99.414285714286%\n",
       "{\n",
       "  _mat_flat : LongTensor - size: 100\n",
       "  valids : FloatTensor - size: 10\n",
       "  mat : LongTensor - size: 10x10\n",
       "  averageUnionValid : 0.98833605647087\n",
       " "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       " _targ_idx : LongTensor - empty\n",
       "  averageValid : 0.99414548873901\n",
       "  classes : \n",
       "    {\n",
       "      1 : 0\n",
       "      2 : 1\n",
       "      3 : 2\n",
       "      4 : 3\n",
       "      5 : 4\n",
       "      6 : 5\n",
       "      7 : 6\n",
       "      8 : 7\n",
       "      9 : 8\n",
       "      10 : 9\n",
       "    }\n",
       "  _prediction : FloatTensor - size: 10\n",
       "  _pred_idx : LongTensor - size: 1\n",
       "  nclasses : 10\n",
       "  _max : FloatTensor - size: 1\n",
       "  _target : FloatTensor - empty\n",
       "  unionvalids : FloatTensor - size: 10\n",
       "  totalValid : 0.99414285714286\n",
       "}\n"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "==> saving model to /Users/sameh/Downloads/My Work/Kaggle - MNIST Digit Recognizer/itorch_output/model.net\t\n"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "==> Training on 42000 samples\t\n"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "==> doing epoch on training data:\t\n",
       "==> online epoch # 7 [batchSize = 1]\t\n"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [=============================================>]                               \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b Tot: 8m15s | Step: 11ms\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b "
     ]
    },
    {
     "data": {
      "text/plain": [
       "\n",
       "==> time to learn 1 sample = 11.821112740607ms\t\n",
       "ConfusionMatrix:\n",
       "[[    4122       0       0       0       1       0       4       0       3       2]   99.758% \t[class: 0]\n",
       " [       0    4666       6       0       1       0       1       7       3       0]   99.616% \t[class: 1]\n",
       " [       1       1    4155       4       1       0       0      10       4       1]   99.473% \t[class: 2]\n",
       " [       1       0       5    4331       0       3       0       3       4       4]   99.540% \t[class: 3]\n",
       " [       0       1       0       0    4052       0       4       2       1      12]   99.509% \t[class: 4]\n",
       " [       1       0       0       2       0    3781       4       0       5       2]   99.631% \t[class: 5]\n",
       " [       3       0       1       1       2       3    4123       0       4       0]   99.662% \t[class: 6]\n",
       " [       0       7       8       0       5       0       0    4375       3       3]   99.409% \t[class: 7]\n",
       " [       0       2       7       2       3       5       2       1    4031      10]   99.212% \t[class: 8]\n",
       " [       3       3       0       0      12       3       0      10       8    4149]]  99.069% \t[class: 9]\n",
       " + average row correct: 99.487925171852% \n",
       " + average rowUcol correct (VOC measure): 98.98028075695% \n",
       " + global correct: 99.488095238095%\n",
       "{\n",
       "  _mat_flat : LongTensor - size: 100\n",
       "  valids : FloatTensor - size: 10\n",
       "  mat : LongTensor - size: 10x10\n",
       "  averageUnionValid : 0.9898028075695\n",
       "  _targ_idx : LongTensor - empty\n",
       "  averageValid : 0.99487925171852\n",
       "  classes : \n",
       "    {\n",
       "      1 : 0\n",
       "      2 : 1\n",
       "      3 : 2\n",
       "      4 : 3\n",
       "    "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "  5 : 4\n",
       "      6 : 5\n",
       "      7 : 6\n",
       "      8 : 7\n",
       "      9 : 8\n",
       "      10 : 9\n",
       "    }\n",
       "  _prediction : FloatTensor - size: 10\n",
       "  _pred_idx : LongTensor - size: 1\n",
       "  nclasses : 10\n",
       "  _max : FloatTensor - size: 1\n",
       "  _target : FloatTensor - empty\n",
       "  unionvalids : FloatTensor - size: 10\n",
       "  totalValid : 0.99488095238095\n",
       "}\n"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "==> saving model to /Users/sameh/Downloads/My Work/Kaggle - MNIST Digit Recognizer/itorch_output/model.net\t\n"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "==> Training on 42000 samples\t\n",
       "==> doing epoch on training data:\t\n",
       "==> online epoch # 8 [batchSize = 1]\t\n"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [=============================================>]                               \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b Tot: 10m25s | Step: 26ms\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b "
     ]
    },
    {
     "data": {
      "text/plain": [
       "\n",
       "==> time to learn 1 sample = 14.90360726629ms\t\n",
       "ConfusionMatrix:\n",
       "[[    4126       0       1       0       1       0       2       0       1       1]   99.855% \t[class: 0]\n",
       " [       0    4669       4       0       0       0       2       7       2       0]   99.680% \t[class: 1]\n",
       " [       0       1    4159       2       1       0       0      11       2       1]   99.569% \t[class: 2]\n",
       " [       1       0       4    4332       0       4       0       2       4       4]   99.563% \t[class: 3]\n",
       " [       0       1       0       0    4054       0       2       2       1      12]   99.558% \t[class: 4]\n",
       " [       1       0       0       2       0    3784       4       0       2       2]   99.710% \t[class: 5]\n",
       " [       2       1       1       1       1       4    4125       0       2       0]   99.710% \t[class: 6]\n",
       " [       0       8       8       1       4       0       0    4375       0       5]   99.409% \t[class: 7]\n",
       " [       0       1       5       3       0       4       1       0    4040       9]   99.434% \t[class: 8]\n",
       " [       4       1       1       0      10       3       0       7       5    4157]]  99.260% \t[class: 9]\n",
       " + average row correct: 99.574790000916% \n",
       " + average rowUcol correct (VOC measure): 99.152089953423% \n",
       " + global correct: 99.57380952381%\n",
       "{\n",
       "  _mat_flat : LongTensor - size: 100\n",
       "  valids : FloatTensor - size: 10\n",
       "  mat : LongTensor - size: 10x10\n",
       "  averageUnionValid : 0.99152089953423\n",
       "  _targ_idx : LongTensor - empty\n"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "  averageValid : 0.99574790000916\n",
       "  classes : \n",
       "    {\n",
       "      1 : 0\n",
       "      2 : 1\n",
       "      3 : 2\n",
       "      4 : 3\n",
       "      5 : 4\n",
       "      6 : 5\n",
       "      7 : 6\n",
       "      8 : 7\n",
       "      9 : 8\n",
       "      10 : 9\n",
       "    }\n",
       "  _prediction : FloatTensor - size: 10\n",
       "  _pred_idx : LongTensor - size: 1\n",
       "  nclasses : 10\n",
       "  _max : FloatTensor - size: 1\n",
       "  _target : FloatTensor - empty\n",
       "  unionvalids : FloatTensor - size: 10\n",
       "  totalValid : 0.9957380952381\n",
       "}\n"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "==> saving model to /Users/sameh/Downloads/My Work/Kaggle - MNIST Digit Recognizer/itorch_output/model.net\t\n"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "==> Training on 42000 samples\t\n"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "==> doing epoch on training data:\t\n",
       "==> online epoch # 9 [batchSize = 1]\t\n"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [=============================================>]                               \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b Tot: 15m47s | Step: 21ms\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b "
     ]
    },
    {
     "data": {
      "text/plain": [
       "\n",
       "==> time to learn 1 sample = 22.581600189209ms\t\n"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "ConfusionMatrix:\n",
       "[[    4125       0       1       0       0       0       3       0       2       1]   99.831% \t[class: 0]\n",
       " [       0    4669       5       0       1       0       2       6       1       0]   99.680% \t[class: 1]\n",
       " [       0       0    4166       2       1       0       0       5       1       2]   99.737% \t[class: 2]\n",
       " [       0       0       3    4332       0       6       0       2       4       4]   99.563% \t[class: 3]\n",
       " [       0       1       0       0    4057       0       2       1       3       8]   99.632% \t[class: 4]\n",
       " [       0       0       0       2       0    3788       3       0       1       1]   99.816% \t[class: 5]\n",
       " [       3       1       0       1       1       3    4125       0       3       0]   99.710% \t[class: 6]\n",
       " [       0       6       6       0       4       0       0    4381       1       3]   99.546% \t[class: 7]\n",
       " [       1       1       6       2       0       3       1       1    4041       7]   99.459% \t[class: 8]\n",
       " [       3       1       1       0       8       2       0       7       4    4162]]  99.379% \t[class: 9]\n",
       " + average row correct: 99.635069966316% \n",
       " + average rowUcol correct (VOC measure): 99.269458651543% \n",
       " + global correct: 99.633333333333%\n",
       "{\n",
       "  _mat_flat : LongTensor - size: 100\n",
       "  valids : FloatTensor - size: 10\n",
       "  mat : LongTensor - size: 10x10\n",
       "  averageUnionValid : 0.99269458651543\n",
       "  _targ_idx : LongTensor - empty\n",
       "  averageValid : 0.99635069966316\n",
       "  classes : \n",
       "    {\n",
       "      1 : 0\n",
       "      2 : 1\n",
       "      3 : 2\n",
       "      4 : 3\n",
       "      5 : 4\n",
       "      6 : 5\n",
       "      7 : 6\n",
       "      8 : 7\n",
       "      9 : 8\n",
       "      10 : 9\n",
       "    }\n",
       "  _prediction : FloatTensor - size: 10\n",
       "  _pred_idx : LongTensor - size: 1\n",
       "  nclasses : 10\n",
       "  _max : FloatTensor - size: 1\n",
       "  _target : FloatTensor - empty\n",
       "  unionvalids : FloatTensor - size: 10\n",
       "  totalValid : 0.99633333333333\n",
       "}\n"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "==> saving model to /Users/sameh/Downloads/My Work/Kaggle - MNIST Digit Recognizer/itorch_output/model.net\t\n"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "==> Training on 42000 samples\t\n"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "==> doing epoch on training data:\t\n",
       "==> online epoch # 10 [batchSize = 1]\t\n"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [=============================================>]                               \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b Tot: 7m49s | Step: 11ms\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b "
     ]
    },
    {
     "data": {
      "text/plain": [
       "\n",
       "==> time to learn 1 sample = 11.189762927237ms\t\n",
       "ConfusionMatrix:\n",
       "[[    4128       0       0       0       1       0       2       0       1       0]   99.903% \t[class: 0]\n",
       " [       0    4672       2       0       1       0       1       7       1       0]   99.744% \t[class: 1]\n",
       " [       0       0    4169       1       1       0       0       4       1       1]   99.808% \t[class: 2]\n",
       " [       1       0       3    4332       0       5       0       2       4       4]   99.563% \t[class: 3]\n",
       " [       0       2       0       0    4057       0       1       1       1      10]   99.632% \t[class: 4]\n",
       " [       0       0       0       0       0    3790       3       0       1       1]   99.868% \t[class: 5]\n",
       " [       1       1       0       1       1       2    4131       0       0       0]   99.855% \t[class: 6]\n",
       " [       0       6       4       0       3       0       0    4383       0       5]   99.591% \t[class: 7]\n",
       " [       0       2       3       2       1       1       1       0    4046       7]   99.582% \t[class: 8]\n",
       " [       1       0       1       1      11       2       0       5       4    4163]]  99.403% \t[class: 9]\n",
       " + average row correct: 99.694929122925% \n",
       " + average rowUcol correct (VOC measure): 99.389536976814% \n",
       " + global correct: 99.692857142857%\n",
       "{\n",
       "  _mat_flat : LongTensor - size: 100\n",
       "  valids : FloatTensor - size: 10\n",
       "  mat : LongTensor - size: 10x10\n",
       "  averageUnionValid : 0.99389536976814\n",
       "  _targ_idx : LongTensor - empty\n",
       "  averageValid : 0.99694929122925\n",
       "  classes : \n",
       "    {\n",
       "      1 : 0\n",
       "      2 : 1\n",
       "      3 : 2\n",
       "      4 : 3\n",
       "      5 : 4\n",
       "      6 : 5\n",
       "      7 : 6\n",
       "      8 : 7\n",
       "      9 : 8\n",
       "      10 : 9\n",
       "    }\n",
       "  _prediction : FloatTensor - size: 10\n",
       "  _pred_idx : LongTensor - size: 1\n",
       "  nclasses : 10\n",
       "  _max : FloatTensor - size: 1\n",
       "  _target : FloatTensor - empty\n",
       "  unionvalids : FloatTensor - size: 10\n",
       "  totalValid : 0.99692857142857\n",
       "}\n"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "==> saving model to /Users/sameh/Downloads/My Work/Kaggle - MNIST Digit Recognizer/itorch_output/model.net\t\n"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "==> Training on 42000 samples\t\n",
       "==> doing epoch on training data:\t\n",
       "==> online epoch # 11 [batchSize = 1]\t\n"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [=============================================>]                               \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b Tot: 7m48s | Step: 11ms\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b "
     ]
    },
    {
     "data": {
      "text/plain": [
       "\n",
       "==> time to learn 1 sample = 11.188308500108ms\t\n",
       "ConfusionMatrix:\n",
       "[[    4129       0       1       0       0       0       1       0       1       0]   99.927% \t[class: 0]\n",
       " [       0    4675       1       0       1       0       1       6       0       0]   99.808% \t[class: 1]\n",
       " [       0       0    4169       1       0       0       0       5       1       1]   99.808% \t[class: 2]\n",
       " [       0       0       1    4345       0       1       0       0       1       3]   99.862% \t[class: 3]\n",
       " [       0       1       0       0    4060       0       1       1       0       9]   99.705% \t[class: 4]\n",
       " [       0       0       0       1       0    3789       2       0       1       2]   99.842% \t[class: 5]\n",
       " [       3       2       0       1       0       2    4128       0       1       0]   99.782% \t[class: 6]\n",
       " [       0       6       4       0       3       0       0    4385       0       3]   99.636% \t[class: 7]\n",
       " [       0       1       2       2       0       2       1       0    4048       7]   99.631% \t[class: 8]\n",
       " [       1       0       1       1       7       1       0       5       3    4169]]  99.546% \t[class: 9]\n",
       " + average row correct: 99.75490629673% \n",
       " + average rowUcol correct (VOC measure): 99.51232790947% \n",
       " + global correct: 99.754761904762%\n",
       "{\n",
       "  _mat_flat : LongTensor - size: 100\n",
       "  valids : FloatTensor - size: 10\n",
       "  mat : LongTensor - size: 10x10\n",
       "  averageUnionValid : 0.9951232790947\n",
       "  _targ_idx : LongTensor - empty\n",
       "  averageValid : 0.9975490629673\n",
       "  classes : \n",
       "    {\n",
       "      1 : 0\n",
       "      2 : 1\n",
       "      3 : 2\n",
       "      4 : 3\n",
       "      5 : 4\n",
       "      6 : 5\n",
       "      7 : 6\n",
       "      8 : 7\n",
       "      9 : 8\n",
       "      10 : 9\n",
       "    }\n",
       "  _prediction : FloatTensor - size: 10\n",
       "  _pred_idx : LongTensor - size: 1\n",
       "  nclasses : 10\n",
       "  _max : FloatTensor - size: 1\n",
       "  _target : FloatTensor - empty\n",
       "  unionvalids : FloatTensor - size: 10\n",
       "  totalValid : 0.99754761904762\n",
       "}\n"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "==> saving model to /Users/sameh/Downloads/My Work/Kaggle - MNIST Digit Recognizer/itorch_output/model.net\t\n"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "==> Training on 42000 samples\t\n"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "==> doing epoch on training data:\t\n",
       "==> online epoch # 12 [batchSize = 1]\t\n"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [=============================================>]                               \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b Tot: 19m27s | Step: 13ms\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b "
     ]
    },
    {
     "data": {
      "text/plain": [
       "\n",
       "==> time to learn 1 sample = 27.815351332937ms\t\n"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "ConfusionMatrix:\n",
       "[[    4130       0       0       0       0       0       1       0       1       0]   99.952% \t[class: 0]\n",
       " [       0    4676       1       0       0       0       1       6       0       0]   99.829% \t[class: 1]\n",
       " [       0       0    4173       0       0       0       0       3       1       0]   99.904% \t[class: 2]\n",
       " [       0       0       1    4343       0       2       0       1       1       3]   99.816% \t[class: 3]\n",
       " [       0       1       0       0    4059       0       1       1       1       9]   99.681% \t[class: 4]\n",
       " [       0       0       0       0       0    3793       1       0       1       0]   99.947% \t[class: 5]\n",
       " [       2       0       0       0       1       1    4131       0       2       0]   99.855% \t[class: 6]\n",
       " [       0       7       1       0       1       0       0    4390       0       2]   99.750% \t[class: 7]\n",
       " [       0       0       2       2       0       2       1       0    4050       6]   99.680% \t[class: 8]\n",
       " [       2       0       0       0       6       1       0       5       3    4171]]  99.594% \t[class: 9]\n",
       " + average row correct: 99.800835847855% \n",
       " + average rowUcol correct (VOC measure): 99.602299928665% \n",
       " + global correct: 99.8%\n",
       "{\n",
       "  _mat_flat : LongTensor - size: 100\n",
       "  valids : FloatTensor - size: 10\n",
       "  mat : LongTensor - size: 10x10\n",
       "  averageUnionValid : 0.99602299928665\n",
       "  _targ_idx : LongTensor - empty\n",
       "  averageValid : 0.99800835847855\n",
       "  classes : \n",
       "    {\n",
       "      1 : 0\n",
       "      2 : 1\n",
       "      3 : 2\n",
       "      4 : 3\n",
       "      5 : 4\n",
       "      6 : 5\n",
       "      7 : 6\n",
       "      8 : 7\n",
       "      9 : 8\n",
       "      10 : 9\n",
       "    }\n",
       "  _prediction : FloatTensor - size: 10\n",
       "  _pred_idx : LongTensor - size: 1\n",
       "  nclasses : 10\n",
       "  _max : FloatTensor - size: 1\n",
       "  _target : FloatTensor - empty\n",
       "  unionvalids : FloatTensor - size: 10\n",
       "  totalValid : 0.998\n",
       "}\n"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "==> saving model to /Users/sameh/Downloads/My Work/Kaggle - MNIST Digit Recognizer/itorch_output/model.net\t\n"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "==> Training on 42000 samples\t\n"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "==> doing epoch on training data:\t\n",
       "==> online epoch # 13 [batchSize = 1]\t\n"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [=============================================>]                               \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b Tot: 8m37s | Step: 12ms\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b "
     ]
    },
    {
     "data": {
      "text/plain": [
       "\n",
       "==> time to learn 1 sample = 12.349780094056ms\t\n",
       "ConfusionMatrix:\n",
       "[[    4128       0       1       0       0       0       1       0       2       0]   99.903% \t[class: 0]\n",
       " [       0    4676       1       0       1       0       0       5       1       0]   99.829% \t[class: 1]\n",
       " [       0       0    4173       1       0       0       0       2       1       0]   99.904% \t[class: 2]\n",
       " [       0       0       1    4347       0       1       0       1       0       1]   99.908% \t[class: 3]\n",
       " [       0       1       0       0    4059       0       1       1       1       9]   99.681% \t[class: 4]\n",
       " [       0       0       0       0       0    3794       1       0       0       0]   99.974% \t[class: 5]\n",
       " [       1       0       0       0       0       1    4135       0       0       0]   99.952% \t[class: 6]\n",
       " [       0       8       1       1       2       0       0    4387       0       2]   99.682% \t[class: 7]\n",
       " [       0       0       2       1       1       2       1       0    4052       4]   99.729% \t[class: 8]\n",
       " [       1       0       0       0       7       1       0       3       1    4175]]  99.690% \t[class: 9]\n",
       " + average row correct: 99.825149774551% \n",
       " + average rowUcol correct (VOC measure): 99.650542140007% \n",
       " + global correct: 99.82380952381%\n",
       "{\n",
       "  _mat_flat : LongTensor - size: 100\n",
       "  valids : FloatTensor - size: 10\n",
       "  mat : LongTensor - size: 10x10\n",
       "  averageUnionValid : 0.99650542140007\n",
       "  _targ_idx : LongTensor - empty\n",
       "  averageValid : 0.99825149774551\n",
       "  classes : \n",
       "    {\n",
       "      1 : 0\n",
       "      2 : 1\n",
       "      3 : 2\n",
       "      4 : 3\n",
       "      5 : 4\n",
       "      6 : 5\n",
       "      7 : 6\n",
       "      8 : 7\n",
       "      9 : 8\n",
       "      10 : 9\n",
       "    }\n",
       "  _prediction : FloatTensor - size: 10\n",
       "  _pred_idx : LongTensor - size: 1\n",
       "  nclasses : 10\n",
       "  _max : FloatTensor - size: 1\n",
       "  _target : FloatTensor - empty\n",
       "  unionvalids : FloatTensor - size: 10\n",
       "  totalValid : 0.9982380952381\n",
       "}\n"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "==> saving model to /Users/sameh/Downloads/My Work/Kaggle - MNIST Digit Recognizer/itorch_output/model.net\t\n"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "==> Training on 42000 samples\t\n"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "==> doing epoch on training data:\t\n",
       "==> online epoch # 14 [batchSize = 1]\t\n"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [=============================================>]                               \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b Tot: 8m9s | Step: 11ms\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b "
     ]
    },
    {
     "data": {
      "text/plain": [
       "\n",
       "==> time to learn 1 sample = 11.666481307575ms\t\n"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "ConfusionMatrix:\n",
       "[[    4131       0       0       0       0       0       0       0       1       0]   99.976% \t[class: 0]\n",
       " [       0    4676       1       0       0       0       1       5       1       0]   99.829% \t[class: 1]\n",
       " [       0       0    4175       0       0       0       0       1       1       0]   99.952% \t[class: 2]\n",
       " [       0       0       0    4348       0       1       0       0       0       2]   99.931% \t[class: 3]\n",
       " [       0       1       0       0    4062       0       1       1       1       6]   99.754% \t[class: 4]\n",
       " [       0       0       0       0       0    3794       1       0       0       0]   99.974% \t[class: 5]\n",
       " [       2       0       0       1       1       1    4132       0       0       0]   99.879% \t[class: 6]\n",
       " [       0       5       3       0       1       0       0    4391       0       1]   99.773% \t[class: 7]\n",
       " [       0       1       3       2       0       1       1       0    4052       3]   99.729% \t[class: 8]\n",
       " [       1       0       0       0       6       1       0       2       2    4176]]  99.713% \t[class: 9]\n",
       " + average row correct: 99.851089715958% \n",
       " + average rowUcol correct (VOC measure): 99.702040553093% \n",
       " + global correct: 99.85%\n",
       "{\n",
       "  _mat_flat : LongTensor - size: 100\n",
       "  valids : FloatTensor - size: 10\n",
       "  mat : LongTensor - size: 10x10\n",
       "  averageUnionValid : 0.99702040553093\n",
       "  _targ_idx : LongTensor - empty\n",
       "  averageValid : 0.99851089715958\n",
       "  classes : \n",
       "    {\n",
       "      1 : 0\n",
       "      2 : 1\n",
       "      3 : 2\n",
       "      4 : 3\n",
       "      5 : 4\n",
       "      6 : 5\n",
       "      7 : 6\n",
       "      8 : 7\n",
       "      9 : 8\n",
       "      10 : 9\n",
       "    }\n",
       "  _prediction : FloatTensor - size: 10\n",
       "  _pred_idx : LongTensor - size: 1\n",
       "  nclasses : 10\n",
       "  _max : FloatTensor - size: 1\n",
       "  _target : FloatTensor - empty\n",
       "  unionvalids : FloatTensor - size: 10\n",
       "  totalValid : 0.9985\n",
       "}\n"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "==> saving model to /Users/sameh/Downloads/My Work/Kaggle - MNIST Digit Recognizer/itorch_output/model.net\t\n"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "==> Training on 42000 samples\t\n"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "==> doing epoch on training data:\t\n",
       "==> online epoch # 15 [batchSize = 1]\t\n"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [====================================>.........]                               \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b ETA: 1m26s | Step: 11ms\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b "
     ]
    }
   ],
   "source": [
    "----------------------------------------------------------------------\n",
    "print '==> training!'\n",
    "\n",
    "for i =1,40 do\n",
    "    train()\n",
    "end\n",
    "\n",
    "----------------------------------------------------------------------\n",
    "print '==> testing!'\n",
    "test()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "iTorch",
   "language": "lua",
   "name": "itorch"
  },
  "language_info": {
   "name": "lua",
   "version": "5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
